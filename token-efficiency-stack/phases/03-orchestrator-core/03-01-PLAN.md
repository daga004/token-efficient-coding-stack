# PLAN 03-01: Orchestrator Complexity Scoring & Model Registry

## Objective
Implement the rule-based complexity scoring system and model registry that determines which model handles each task.

## Context
```
@STATE.md
```

Prior design from conversation "Adaptive model selection framework":
- Complexity 0-10 scale based on task signals
- Route: 0-2 → local, 2-4 → Haiku/Cerebras, 4-7 → Sonnet, 7+ → Opus
- Input/output cost asymmetry: use expensive models for reading/validating, cheap for generating

## Tasks

<task type="auto">
  <n>Initialize orchestrator project structure</n>
  <files>
    orchestrator/
    orchestrator/__init__.py
    orchestrator/scoring.py
    orchestrator/registry.py
    orchestrator/config.py
    pyproject.toml
  </files>
  <action>
    Create Python package with:
    - pyproject.toml with dependencies: httpx, pydantic
    - config.py for model endpoints and API keys (from env vars)
    - Placeholder modules
  </action>
  <verify>python -c "import orchestrator; print('OK')"</verify>
  <done>Project structure exists</done>
</task>

<task type="auto">
  <n>Implement complexity scoring</n>
  <files>orchestrator/scoring.py</files>
  <action>
    COMPLEXITY_SIGNALS = {
        # Task type signals
        "fix": -1, "typo": -2, "rename": -1,           # Simple
        "refactor": +2, "architect": +4, "design": +3,  # Complex
        "debug": +2, "optimize": +2,                    # Medium
        
        # Scope signals  
        "single file": 0, "multiple files": +2, "cross-module": +3,
        
        # Uncertainty signals
        "research": +2, "unknown": +2, "investigate": +2,
    }
    
    def score_complexity(task_description: str, files_count: int = 1, 
                         task_type: str = "auto") -> int:
        score = 3  # Base score
        
        # Keyword matching (case-insensitive)
        for signal, adjustment in COMPLEXITY_SIGNALS.items():
            if signal in task_description.lower():
                score += adjustment
        
        # File count adjustment
        if files_count > 3:
            score += 2
        elif files_count > 1:
            score += 1
            
        # Task type from GSD
        if task_type == "research":
            score += 2
            
        return max(0, min(10, score))  # Clamp 0-10
    
    Include confidence score based on signal match count.
  </action>
  <verify>
    Test cases:
    - "fix typo in readme" → score ≤ 2
    - "refactor auth module across 5 files" → score ≥ 6
    - "add logging" → score 3-4
  </verify>
  <done>Scoring function returns 0-10 with expected distribution</done>
</task>

<task type="auto">
  <n>Implement model registry</n>
  <files>orchestrator/registry.py</files>
  <action>
    from dataclasses import dataclass
    from enum import Enum
    
    class ModelTier(Enum):
        LOCAL = "local"
        FAST_API = "fast_api"    # Cerebras, Groq
        EFFICIENT = "efficient"  # Haiku
        CAPABLE = "capable"      # Sonnet
        PREMIUM = "premium"      # Opus
    
    @dataclass
    class ModelProfile:
        name: str
        tier: ModelTier
        endpoint: str              # API endpoint or "ollama://model-name"
        input_cost_per_1k: float   # $ per 1K input tokens
        output_cost_per_1k: float  # $ per 1K output tokens
        max_output: int            # Recommended max output tokens
        strengths: list[str]       # ["code", "reasoning", "speed"]
    
    MODEL_REGISTRY = {
        "qwen3-30b": ModelProfile(
            name="qwen3-30b-a3b",
            tier=ModelTier.LOCAL,
            endpoint="ollama://qwen3-30b-a3b",
            input_cost_per_1k=0.0,
            output_cost_per_1k=0.0,
            max_output=2000,
            strengths=["code", "speed"]
        ),
        "haiku": ModelProfile(
            name="claude-3-5-haiku-latest",
            tier=ModelTier.EFFICIENT,
            endpoint="https://api.anthropic.com/v1/messages",
            input_cost_per_1k=0.001,
            output_cost_per_1k=0.005,
            max_output=4000,
            strengths=["code", "speed", "cost"]
        ),
        # ... sonnet, opus, cerebras-glm
    }
    
    def select_model(complexity: int, task_type: str = "code") -> ModelProfile:
        if complexity <= 2:
            return MODEL_REGISTRY["qwen3-30b"]
        elif complexity <= 4:
            return MODEL_REGISTRY["haiku"]
        elif complexity <= 7:
            return MODEL_REGISTRY["sonnet"]
        else:
            return MODEL_REGISTRY["opus"]
  </action>
  <verify>
    python -c "from orchestrator.registry import select_model; m = select_model(3); print(m.name)"
  </verify>
  <done>Model registry with tier-based selection working</done>
</task>

## Verification
- [ ] Complexity scoring produces expected scores for test cases
- [ ] Model registry returns appropriate model for each tier
- [ ] Configuration loads API keys from environment

## Success Criteria
10 sample task descriptions scored and routed to expected models.

## Output
Update STATE.md with: scoring test results, model mapping confirmed.
SUMMARY.md only if major issues or stopping.
