# STATE: Token-Efficient AI Coding Stack

## Current Position
- **Phase:** Not started
- **Next action:** Begin Phase 1 (AuZoom Parser Foundation)

## Decisions Made
*None yet — will populate as we progress*

## Technical Choices Queue
Decisions to make during implementation:

1. **Tree-sitter vs AST module** — Tree-sitter for multi-language support, but ast module simpler for Python-only start
2. **MCP SDK version** — Need to verify compatible version with Claude Code
3. **Local model interface** — Ollama API vs direct llama.cpp
4. **Caching strategy** — In-memory vs file-based AST cache

## Blockers
*None currently*

## Learnings Log
*Format: Date | Phase | Learning | Impact*

| Date | Phase | Learning | Impact |
|------|-------|----------|--------|
| — | — | — | — |

## Token/Cost Measurements
*Baseline and improvements tracked here*

| Task | Baseline (Sonnet, full files) | With AuZoom | With Orchestrator | Combined |
|------|------------------------------|-------------|-------------------|----------|
| — | — | — | — | — |

## Context for Next Session
*Updated at end of each session*

Starting fresh. Prior work exists in two Claude.ai conversations:
- "Reducing AI agent token usage through selective code parsing" — AuZoom spec
- "Adaptive model selection framework for cost-optimized AI agents" — Orchestrator design

Reference these for detailed specifications.
