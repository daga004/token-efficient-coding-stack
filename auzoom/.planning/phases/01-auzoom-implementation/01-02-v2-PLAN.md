---
phase: 01-auzoom-implementation
plan: 02-v2
type: execute
supersedes: 01-02-PLAN.md
---

<objective>
Implement LazyCodeGraph that indexes files on-demand with persistent caching and LLM-based smart invalidation.

Purpose: Enable zero-startup-cost code navigation that scales to massive codebases by parsing only what's needed, when it's needed.

Output: LazyCodeGraph with JSON-based cache, entry point discovery, and intelligent summary update detection using LLM.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-phase.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-auzoom-implementation/01-01-SUMMARY.md

**Existing code**:
@auzoom/src/auzoom/models.py
@auzoom/src/auzoom/parser.py

**Design decisions**:
- Q1: Return cached if exists, otherwise parse in background and return placeholder
- Q2: Skip non-parseable files for V1
- Q3: Simple JSON files in .auzoom/metadata/, no database
- Q4: Use LLM to check if diff requires summary update
- Q5: User guides agent if entry point unclear

**Key innovation**: Lazy-first architecture from ground up, even for Python files.
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement LazyCodeGraph with JSON cache</name>
  <files>auzoom/src/auzoom/lazy_graph.py</files>
  <action>
Create LazyCodeGraph that parses files on-demand and caches to disk:

```python
from pathlib import Path
import json
import hashlib
from typing import Optional, Union
from datetime import datetime
from .models import CodeNode, FetchLevel, NodeType
from .parser import PythonParser


class LazyCodeGraph:
    """Graph that indexes files on-demand with persistent caching."""

    def __init__(self, project_root: str):
        self.project_root = Path(project_root).resolve()
        self.cache_dir = self.project_root / ".auzoom"
        self.metadata_dir = self.cache_dir / "metadata"
        self.metadata_dir.mkdir(parents=True, exist_ok=True)

        self.index = self._load_index()
        self.nodes = {}  # node_id → CodeNode (only loaded nodes)
        self.file_index = {}  # file_path → [node_ids] (only loaded files)
        self.parser = PythonParser()

        # Stats for optimization
        self.stats = {"cache_hits": 0, "cache_misses": 0, "parses": 0}

    def _load_index(self) -> dict:
        """Load or create index.json."""
        index_path = self.cache_dir / "index.json"
        if index_path.exists():
            return json.loads(index_path.read_text())
        return {}

    def _save_index(self):
        """Persist index to disk."""
        (self.cache_dir / "index.json").write_text(
            json.dumps(self.index, indent=2)
        )

    def _compute_hash(self, file_path: Union[str, Path]) -> str:
        """Content hash for cache key."""
        content = Path(file_path).read_bytes()
        return hashlib.sha256(content).hexdigest()[:8]

    def _timestamp(self) -> str:
        """ISO timestamp."""
        return datetime.utcnow().isoformat() + "Z"

    def get_file(self, file_path: str, level: FetchLevel) -> list[dict]:
        """Get file nodes, parsing lazily if needed.

        Flow:
        1. Check memory cache
        2. Check disk cache (validate hash)
        3. Parse and cache if needed
        """
        file_path = str(Path(file_path).resolve())

        # 1. Already in memory?
        if self._is_loaded(file_path):
            self.stats["cache_hits"] += 1
            return self._serialize_file(file_path, level)

        # 2. On disk with valid hash?
        cached = self._load_from_cache(file_path)
        if cached:
            self.stats["cache_hits"] += 1
            self._hydrate_nodes(cached)
            return self._serialize_file(file_path, level)

        # 3. Parse now (first access or stale)
        self.stats["cache_misses"] += 1
        self._parse_and_cache(file_path)
        return self._serialize_file(file_path, level)

    def _is_loaded(self, file_path: str) -> bool:
        """Check if file's nodes are in memory."""
        return file_path in self.file_index

    def _load_from_cache(self, file_path: str) -> Optional[dict]:
        """Try to load from disk cache if hash matches."""
        if file_path not in self.index:
            return None

        entry = self.index[file_path]
        if not entry.get("indexed"):
            return None

        # Validate content hasn't changed
        try:
            current_hash = self._compute_hash(file_path)
        except FileNotFoundError:
            return None

        if current_hash != entry["hash"]:
            # File changed, check if summary needs update
            if self._should_update_summary(file_path, entry):
                return None  # Force re-parse
            # Content changed but summary still valid
            entry["hash"] = current_hash
            self._save_index()

        # Load metadata
        cache_file = self.metadata_dir / f"{file_path.replace('/', '_')}_{entry['hash']}.json"
        if cache_file.exists():
            return json.loads(cache_file.read_text())

        return None

    def _should_update_summary(self, file_path: str, old_entry: dict) -> bool:
        """Use LLM to determine if summary needs updating.

        This will be implemented in Task 2 with LLM integration.
        For now, return True (always re-parse on content change).
        """
        return True

    def _parse_and_cache(self, file_path: str):
        """Parse file and cache metadata to disk."""
        self.stats["parses"] += 1

        # Parse file
        nodes = self.parser.parse_file(file_path)

        # Store in memory
        node_ids = []
        for node in nodes:
            self.nodes[node.id] = node
            node_ids.append(node.id)

        self.file_index[file_path] = node_ids

        # Extract imports for graph expansion
        imports = self._extract_imports(nodes)

        # Cache to disk
        content_hash = self._compute_hash(file_path)
        cache_data = {
            "file_path": file_path,
            "hash": content_hash,
            "indexed_at": self._timestamp(),
            "nodes": [self._serialize_node_for_cache(n) for n in nodes],
            "imports": imports
        }

        cache_file = self.metadata_dir / f"{file_path.replace('/', '_')}_{content_hash}.json"
        cache_file.write_text(json.dumps(cache_data, indent=2))

        # Update index
        self.index[file_path] = {
            "hash": content_hash,
            "indexed": True,
            "indexed_at": self._timestamp(),
            "imports": imports,
            "node_count": len(nodes)
        }
        self._save_index()

        # Discover imports (but don't parse them)
        self._discover_imports(imports)

    def _extract_imports(self, nodes: list[CodeNode]) -> list[str]:
        """Get list of imported file paths from nodes."""
        imports = []
        for node in nodes:
            if node.node_type == NodeType.IMPORT:
                # Extract import path from source
                # e.g., "from .parser import PythonParser" → "parser"
                resolved = self._resolve_import(node.name, node.file_path)
                if resolved:
                    imports.append(resolved)
        return imports

    def _resolve_import(self, import_name: str, from_file: str) -> Optional[str]:
        """Convert import name to file path.

        Simple resolution for V1:
        - Relative imports: resolve relative to from_file
        - Absolute imports: resolve from project root
        """
        # Remove "from" and "import" keywords if present
        import_name = import_name.replace("from ", "").replace("import ", "").split()[0]

        # Handle relative imports
        if import_name.startswith("."):
            base_dir = Path(from_file).parent
            import_path = import_name.lstrip(".")
            potential = base_dir / f"{import_path}.py"
        else:
            # Absolute from project root
            import_path = import_name.replace(".", "/")
            potential = self.project_root / "src" / f"{import_path}.py"
            if not potential.exists():
                potential = self.project_root / f"{import_path}.py"

        if potential.exists():
            return str(potential.resolve())

        return None

    def _discover_imports(self, imports: list[str]):
        """Add imports to index as 'discovered but not indexed'."""
        for imp in imports:
            if imp not in self.index:
                self.index[imp] = {
                    "hash": None,
                    "indexed": False,
                    "discovered_at": self._timestamp()
                }
        self._save_index()

    def _serialize_node_for_cache(self, node: CodeNode) -> dict:
        """Serialize node for disk cache."""
        return {
            "id": node.id,
            "name": node.name,
            "node_type": node.node_type.value,
            "file_path": node.file_path,
            "line_start": node.line_start,
            "line_end": node.line_end,
            "dependencies": node.dependencies,
            "children": node.children,
            "docstring": node.docstring,
            "signature": node.signature,
            "source": node.source
        }

    def _hydrate_nodes(self, cache_data: dict):
        """Load nodes from cache into memory."""
        file_path = cache_data["file_path"]
        node_ids = []

        for node_data in cache_data["nodes"]:
            node = CodeNode(
                id=node_data["id"],
                name=node_data["name"],
                node_type=NodeType(node_data["node_type"]),
                file_path=node_data["file_path"],
                line_start=node_data["line_start"],
                line_end=node_data["line_end"],
                dependencies=node_data["dependencies"],
                children=node_data["children"],
                docstring=node_data.get("docstring"),
                signature=node_data.get("signature"),
                source=node_data.get("source")
            )
            self.nodes[node.id] = node
            node_ids.append(node.id)

        self.file_index[file_path] = node_ids

    def _serialize_file(self, file_path: str, level: FetchLevel) -> list[dict]:
        """Return nodes for a file at requested fetch level."""
        node_ids = self.file_index.get(file_path, [])
        nodes = [self.nodes[nid] for nid in node_ids]

        if level == FetchLevel.SKELETON:
            return [n.to_skeleton() for n in nodes]
        elif level == FetchLevel.SUMMARY:
            return [n.to_summary() for n in nodes]
        else:  # FULL
            return [n.to_full() for n in nodes]

    def get_node(self, node_id: str, level: FetchLevel) -> dict:
        """Get single node, parsing file if needed."""
        # If not in memory, need to load the file
        if node_id not in self.nodes:
            # Extract file path from node ID
            file_path = node_id.split("::")[0]
            self.get_file(file_path, level)

        node = self.nodes.get(node_id)
        if not node:
            raise KeyError(f"Node {node_id} not found")

        if level == FetchLevel.SKELETON:
            return node.to_skeleton()
        elif level == FetchLevel.SUMMARY:
            return node.to_summary()
        else:
            return node.to_full()

    def get_children(self, node_id: str, level: FetchLevel) -> list[dict]:
        """Get child nodes."""
        node = self.nodes.get(node_id)
        if not node:
            return []
        return [self.get_node(cid, level) for cid in node.children]

    def get_dependencies(self, node_id: str, depth: int = 1) -> list[dict]:
        """Get dependency nodes, loading files as needed."""
        if depth < 1:
            return []

        visited = set()
        result = []

        def traverse(nid, d):
            if d > depth or nid in visited:
                return
            visited.add(nid)

            # Ensure node is loaded
            if nid not in self.nodes:
                try:
                    self.get_node(nid, FetchLevel.SKELETON)
                except KeyError:
                    return

            node = self.nodes.get(nid)
            if not node:
                return

            for dep_id in node.dependencies:
                if dep_id not in visited:
                    try:
                        result.append(self.get_node(dep_id, FetchLevel.SKELETON))
                        traverse(dep_id, d + 1)
                    except KeyError:
                        pass  # Dependency not found

        traverse(node_id, 1)
        return result

    def find_by_name(self, name_pattern: str) -> list[dict]:
        """Search across all loaded nodes."""
        matches = []
        for node in self.nodes.values():
            if name_pattern.lower() in node.name.lower():
                matches.append(node.to_skeleton())
        return matches

    def get_discovered_files(self) -> list[dict]:
        """List files discovered via imports but not yet indexed."""
        return [
            {"path": path, "discovered_at": entry["discovered_at"]}
            for path, entry in self.index.items()
            if not entry.get("indexed")
        ]

    def get_stats(self) -> dict:
        """Return cache performance stats."""
        total = self.stats["cache_hits"] + self.stats["cache_misses"]
        hit_rate = self.stats["cache_hits"] / total if total > 0 else 0

        return {
            "cache_hits": self.stats["cache_hits"],
            "cache_misses": self.stats["cache_misses"],
            "hit_rate": f"{hit_rate:.1%}",
            "files_parsed": self.stats["parses"],
            "files_indexed": len([e for e in self.index.values() if e.get("indexed")]),
            "files_discovered": len([e for e in self.index.values() if not e.get("indexed")]),
            "nodes_in_memory": len(self.nodes)
        }
```

Key features:
- Zero startup cost (no indexing on init)
- Persistent JSON cache with content hashing
- Memory cache for loaded files
- Import discovery without parsing
- Stats tracking for optimization

  </action>
  <verify>
PYTHONPATH=auzoom/src:$PYTHONPATH python3 -c "
from auzoom.lazy_graph import LazyCodeGraph
from auzoom.models import FetchLevel
import time

g = LazyCodeGraph('.')

# First access (cold)
start = time.time()
nodes1 = g.get_file('auzoom/src/auzoom/models.py', FetchLevel.SKELETON)
cold_time = time.time() - start

# Second access (warm)
start = time.time()
nodes2 = g.get_file('auzoom/src/auzoom/models.py', FetchLevel.SKELETON)
warm_time = time.time() - start

print(f'Cold: {cold_time:.3f}s, Warm: {warm_time:.3f}s')
print(f'Speedup: {cold_time/warm_time:.1f}x')
print(f'Stats: {g.get_stats()}')
assert warm_time < cold_time / 10, 'Cache should be 10x+ faster'
"
  </verify>
  <done>LazyCodeGraph working with persistent cache, 10x+ speedup on warm access</done>
</task>

<task type="auto">
  <name>Task 2: Implement LLM-based smart invalidation</name>
  <files>auzoom/src/auzoom/lazy_graph.py</files>
  <action>
Add LLM integration for intelligent summary update detection:

```python
def _should_update_summary(self, file_path: str, old_entry: dict) -> bool:
    """Use LLM to determine if file changes warrant summary update.

    Args:
        file_path: Path to changed file
        old_entry: Previous index entry with summary

    Returns:
        True if summary should be regenerated
    """
    try:
        # Load old cached data
        old_cache_file = self.metadata_dir / f"{file_path.replace('/', '_')}_{old_entry['hash']}.json"
        if not old_cache_file.exists():
            return True  # No old cache, need new summary

        old_cache = json.loads(old_cache_file.read_text())

        # Get file diff
        old_content = self._get_cached_source(old_cache)
        new_content = Path(file_path).read_text()

        # Compute diff
        import difflib
        diff = '\n'.join(difflib.unified_diff(
            old_content.splitlines(),
            new_content.splitlines(),
            lineterm='',
            n=3  # 3 lines of context
        ))

        if not diff:
            return False  # No changes

        # Call LLM for semantic analysis
        return self._ask_llm_about_summary(old_entry, diff)

    except Exception as e:
        # On error, conservatively re-parse
        print(f"Warning: Error checking summary validity: {e}")
        return True

def _get_cached_source(self, cache_data: dict) -> str:
    """Reconstruct source from cached nodes."""
    nodes = cache_data.get("nodes", [])
    if not nodes:
        return ""

    # Sort by line number and concatenate sources
    nodes_sorted = sorted(nodes, key=lambda n: n.get("line_start", 0))
    return "\n\n".join(n.get("source", "") for n in nodes_sorted if n.get("source"))

def _ask_llm_about_summary(self, old_entry: dict, diff: str) -> bool:
    """Query LLM: does this diff require summary update?

    Uses a small, fast model (haiku) for cost efficiency.
    """
    # Generate current summary from old entry
    old_summary = self._generate_file_summary(old_entry)

    prompt = f"""You are analyzing code changes to determine if a cached summary needs updating.

OLD SUMMARY:
{old_summary}

CODE DIFF:
{diff}

QUESTION: Does this diff change the file's behavior or structure enough that the summary above is now inaccurate or misleading?

Consider:
- New/removed functions or classes → YES
- Changed function signatures → YES
- Changed docstrings → MAYBE (if meaning changed)
- Implementation details only → NO
- Formatting/whitespace → NO
- Comments only → NO

Reply with just: YES or NO"""

    try:
        # Use Claude API (haiku for speed/cost)
        import anthropic
        client = anthropic.Anthropic()  # Uses ANTHROPIC_API_KEY env var

        message = client.messages.create(
            model="claude-3-haiku-20240307",
            max_tokens=10,
            temperature=0,
            messages=[{"role": "user", "content": prompt}]
        )

        response = message.content[0].text.strip().upper()
        return "YES" in response

    except Exception as e:
        print(f"Warning: LLM check failed: {e}")
        return True  # Conservative: re-parse on error

def _generate_file_summary(self, index_entry: dict) -> str:
    """Generate summary from index entry."""
    imports = index_entry.get("imports", [])
    node_count = index_entry.get("node_count", 0)

    return f"""File: {index_entry.get('file_path', 'unknown')}
Nodes: {node_count} (functions, classes, methods)
Imports: {len(imports)} files
Last indexed: {index_entry.get('indexed_at', 'unknown')}"""
```

Add dependency to pyproject.toml:
```toml
[tool.poetry.dependencies]
anthropic = "^0.39.0"  # For LLM-based invalidation
```

Environment setup needed:
```bash
export ANTHROPIC_API_KEY="your-key-here"
```

**Important**: Make LLM check optional via config flag for users without API key.

Add to LazyCodeGraph.__init__:
```python
# Optional: disable LLM checks if no API key
import os
self.use_llm_invalidation = os.getenv("ANTHROPIC_API_KEY") is not None
if not self.use_llm_invalidation:
    print("Warning: ANTHROPIC_API_KEY not set, using simple hash-based invalidation")
```

Update _should_update_summary:
```python
def _should_update_summary(self, file_path: str, old_entry: dict) -> bool:
    if not self.use_llm_invalidation:
        return True  # Always re-parse if no LLM

    # ... rest of LLM logic
```

  </action>
  <verify>
# Test with mock diff
PYTHONPATH=auzoom/src:$PYTHONPATH python3 -c "
from auzoom.lazy_graph import LazyCodeGraph
import os

# Set mock API key for testing
os.environ['ANTHROPIC_API_KEY'] = 'test'

g = LazyCodeGraph('.')
print(f'LLM invalidation enabled: {g.use_llm_invalidation}')

# Test would require actual file changes and API key
# Manual testing needed with real scenario
"
  </verify>
  <done>LLM-based smart invalidation implemented, falls back to hash-only if no API key</done>
</task>

<task type="auto">
  <name>Task 3: Add entry point discovery and background warming</name>
  <files>auzoom/src/auzoom/lazy_graph.py</files>
  <action>
Add methods for discovering entry points and pre-warming cache:

```python
def discover_entry_points(self) -> list[str]:
    """Find likely entry points in project.

    Priority order:
    1. Files with if __name__ == "__main__"
    2. Common entry point names (main.py, app.py, etc.)
    3. Files in project root
    """
    candidates = []

    # Check common names in root
    common_names = [
        "main.py", "app.py", "__main__.py",
        "manage.py", "run.py", "server.py", "cli.py"
    ]

    for name in common_names:
        path = self.project_root / name
        if path.exists():
            candidates.append(str(path.resolve()))

    # Scan for __main__ blocks
    for py_file in self.project_root.rglob("*.py"):
        # Skip venv, node_modules, etc.
        if any(part.startswith(".") or part in ["venv", "node_modules", "__pycache__"]
               for part in py_file.parts):
            continue

        try:
            content = py_file.read_text()
            if '__name__ == "__main__"' in content or "__name__ == '__main__'" in content:
                candidates.append(str(py_file.resolve()))
        except:
            pass

    return candidates[:5]  # Limit to top 5

def warm_cache(self, file_paths: list[str], level: FetchLevel = FetchLevel.SKELETON):
    """Pre-parse files to warm cache (non-blocking).

    Args:
        file_paths: List of files to pre-parse
        level: Fetch level to cache
    """
    import threading

    def warm_thread():
        for path in file_paths:
            try:
                self.get_file(path, level)
            except Exception as e:
                print(f"Warning: Failed to warm {path}: {e}")

    thread = threading.Thread(target=warm_thread, daemon=True)
    thread.start()
    return thread

def warm_entry_points(self):
    """Discover and pre-parse entry points in background."""
    entry_points = self.discover_entry_points()

    if not entry_points:
        print("No entry points discovered, cache will warm on-demand")
        return

    print(f"Warming cache for {len(entry_points)} entry points...")
    self.warm_cache(entry_points)

def preload_discovered(self, limit: int = 10):
    """Parse discovered but not-yet-indexed files.

    Args:
        limit: Max number of files to preload
    """
    discovered = self.get_discovered_files()[:limit]
    paths = [f["path"] for f in discovered]

    if paths:
        print(f"Preloading {len(paths)} discovered imports...")
        self.warm_cache(paths)
```

Add to LazyCodeGraph.__init__ (optional auto-warming):
```python
def __init__(self, project_root: str, auto_warm: bool = True):
    # ... existing init code ...

    # Optionally warm cache in background
    if auto_warm:
        import threading
        threading.Thread(
            target=self._auto_warm_sequence,
            daemon=True
        ).start()

def _auto_warm_sequence(self):
    """Background warming strategy."""
    import time

    # Give main thread a head start
    time.sleep(0.1)

    # 1. Warm entry points
    self.warm_entry_points()

    # 2. Wait for entry points to finish discovering imports
    time.sleep(0.5)

    # 3. Warm discovered imports
    self.preload_discovered(limit=10)
```

  </action>
  <verify>
PYTHONPATH=auzoom/src:$PYTHONPATH python3 -c "
from auzoom.lazy_graph import LazyCodeGraph
import time

g = LazyCodeGraph('.', auto_warm=True)

# Give warming thread time to work
time.sleep(2)

stats = g.get_stats()
print(f'Warmed files: {stats[\"files_indexed\"]}')
print(f'Entry points found: {len(g.discover_entry_points())}')

# Verify entry points were actually parsed
entry_points = g.discover_entry_points()
for ep in entry_points[:1]:
    # Should be instant (already warmed)
    start = time.time()
    g.get_file(ep, FetchLevel.SKELETON)
    elapsed = time.time() - start
    print(f'{ep}: {elapsed:.4f}s (should be <0.01s)')
    assert elapsed < 0.01, 'Should be instant from cache'
"
  </verify>
  <done>Entry point discovery and background warming working</done>
</task>

<task type="auto">
  <name>Task 4: Create integration tests for LazyCodeGraph</name>
  <files>tests/test_lazy_graph.py</files>
  <action>
Test lazy loading, caching, and invalidation:

```python
import pytest
import time
from pathlib import Path
from auzoom.lazy_graph import LazyCodeGraph
from auzoom.models import FetchLevel


def test_lazy_loading():
    """Test that files are only parsed on first access."""
    g = LazyCodeGraph('.', auto_warm=False)

    # Initially, no files loaded
    assert g.stats["parses"] == 0

    # First access triggers parse
    nodes = g.get_file('auzoom/src/auzoom/models.py', FetchLevel.SKELETON)
    assert g.stats["parses"] == 1
    assert len(nodes) > 0

    # Second access uses cache
    nodes2 = g.get_file('auzoom/src/auzoom/models.py', FetchLevel.SKELETON)
    assert g.stats["parses"] == 1  # No additional parse
    assert nodes == nodes2


def test_cache_persistence():
    """Test that cache persists across graph instances."""
    # First instance: parse and cache
    g1 = LazyCodeGraph('.', auto_warm=False)
    nodes1 = g1.get_file('auzoom/src/auzoom/models.py', FetchLevel.SKELETON)
    parse_count1 = g1.stats["parses"]
    assert parse_count1 == 1

    # Second instance: should load from disk cache
    g2 = LazyCodeGraph('.', auto_warm=False)
    nodes2 = g2.get_file('auzoom/src/auzoom/models.py', FetchLevel.SKELETON)
    parse_count2 = g2.stats["parses"]

    assert parse_count2 == 0  # No parse needed
    assert len(nodes1) == len(nodes2)


def test_cache_speed():
    """Verify cache is significantly faster than parsing."""
    g = LazyCodeGraph('.', auto_warm=False)

    # Cold access (parse)
    start = time.time()
    g.get_file('auzoom/src/auzoom/models.py', FetchLevel.SKELETON)
    cold_time = time.time() - start

    # Warm access (cache)
    start = time.time()
    g.get_file('auzoom/src/auzoom/models.py', FetchLevel.SKELETON)
    warm_time = time.time() - start

    print(f"Cold: {cold_time:.3f}s, Warm: {warm_time:.3f}s, Speedup: {cold_time/warm_time:.1f}x")
    assert warm_time < cold_time / 5  # Cache should be 5x+ faster


def test_import_discovery():
    """Test that imports are discovered without parsing."""
    g = LazyCodeGraph('.', auto_warm=False)

    # Parse a file that has imports
    g.get_file('auzoom/src/auzoom/parser.py', FetchLevel.SKELETON)

    # Check that imports were discovered
    discovered = g.get_discovered_files()
    assert len(discovered) >= 0  # May discover models.py if not cached

    # Discovered files should not be parsed yet
    for file_info in discovered:
        file_path = file_info["path"]
        assert file_path not in g.file_index or len(g.file_index.get(file_path, [])) == 0


def test_entry_point_discovery():
    """Test entry point discovery."""
    g = LazyCodeGraph('.', auto_warm=False)

    entry_points = g.discover_entry_points()

    print(f"Found entry points: {entry_points}")
    assert isinstance(entry_points, list)
    # May or may not find any, depending on project structure


def test_fetch_levels():
    """Test different fetch levels return appropriate detail."""
    g = LazyCodeGraph('.', auto_warm=False)

    skeleton = g.get_file('auzoom/src/auzoom/models.py', FetchLevel.SKELETON)
    summary = g.get_file('auzoom/src/auzoom/models.py', FetchLevel.SUMMARY)
    full = g.get_file('auzoom/src/auzoom/models.py', FetchLevel.FULL)

    # Verify progressive detail
    assert len(str(skeleton)) < len(str(summary)) < len(str(full))

    # Skeleton should have minimal fields
    if skeleton:
        assert 'id' in skeleton[0]
        assert 'name' in skeleton[0]
        assert 'dependencies' in skeleton[0]

    # Full should have source
    if full:
        assert 'source' in full[0] or full[0].get('node_type') == 'import'


def test_node_access():
    """Test accessing individual nodes lazily."""
    g = LazyCodeGraph('.', auto_warm=False)

    # Get a node by searching first
    g.get_file('auzoom/src/auzoom/models.py', FetchLevel.SKELETON)

    # Find a specific node
    matches = g.find_by_name("CodeNode")
    assert len(matches) > 0

    node_id = matches[0]['id']

    # Access node at different levels
    skeleton = g.get_node(node_id, FetchLevel.SKELETON)
    full = g.get_node(node_id, FetchLevel.FULL)

    assert skeleton['id'] == node_id
    assert full['id'] == node_id
    assert len(str(full)) > len(str(skeleton))


def test_stats_tracking():
    """Test that stats are tracked correctly."""
    g = LazyCodeGraph('.', auto_warm=False)

    # Make some accesses
    g.get_file('auzoom/src/auzoom/models.py', FetchLevel.SKELETON)  # miss
    g.get_file('auzoom/src/auzoom/models.py', FetchLevel.SKELETON)  # hit

    stats = g.get_stats()

    assert stats['cache_hits'] >= 1
    assert stats['cache_misses'] >= 1
    assert stats['files_parsed'] >= 1
    assert 'hit_rate' in stats

    print(f"Stats: {stats}")


def test_file_modification_detection():
    """Test that modified files are detected."""
    import tempfile
    import os

    # Create a temp file
    with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
        f.write('def foo():\n    pass\n')
        temp_path = f.name

    try:
        g = LazyCodeGraph(os.path.dirname(temp_path), auto_warm=False)

        # First access
        nodes1 = g.get_file(temp_path, FetchLevel.SKELETON)
        assert len(nodes1) == 1

        # Modify file
        with open(temp_path, 'w') as f:
            f.write('def foo():\n    pass\n\ndef bar():\n    pass\n')

        # Create new graph instance
        g2 = LazyCodeGraph(os.path.dirname(temp_path), auto_warm=False)

        # Should detect change and re-parse
        nodes2 = g2.get_file(temp_path, FetchLevel.SKELETON)
        assert len(nodes2) == 2  # Now has foo and bar

    finally:
        os.unlink(temp_path)


def test_dependency_traversal_lazy():
    """Test that dependency traversal loads files as needed."""
    test_code = '''
def leaf():
    return 1

def middle():
    return leaf()

def top():
    return middle()
'''
    with open('/tmp/test_lazy_deps.py', 'w') as f:
        f.write(test_code)

    g = LazyCodeGraph('/tmp', auto_warm=False)

    # Parse the file
    g.get_file('/tmp/test_lazy_deps.py', FetchLevel.SKELETON)

    # Get dependencies (should not trigger additional parses)
    nodes = [n for n in g.nodes.values() if n.name == 'top']
    if nodes:
        top_id = nodes[0].id
        deps = g.get_dependencies(top_id, depth=2)

        dep_names = [d['name'] for d in deps]
        assert 'middle' in dep_names
```

Run with: pytest tests/test_lazy_graph.py -v -s
  </action>
  <verify>
cd auzoom && PYTHONPATH=src:$PYTHONPATH python3 -m pytest tests/test_lazy_graph.py -v -s
  </verify>
  <done>All lazy loading tests pass, cache performance validated</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] LazyCodeGraph loads files on-demand (zero startup cost)
- [ ] JSON cache persists across sessions
- [ ] Cache provides 5x+ speedup over parsing
- [ ] LLM-based invalidation works (or gracefully degrades)
- [ ] Entry points discovered and pre-warmed
- [ ] All tests pass
- [ ] .auzoom/ cache directory properly structured
</verification>

<success_criteria>

- All tasks completed
- Zero-cost startup demonstrated (no eager indexing)
- Cache hit rate >80% on second run
- LLM invalidation optional but functional
- Background warming doesn't block user operations
- Tests validate lazy loading behavior

</success_criteria>

<output>
After completion, create `.planning/phases/01-auzoom-implementation/01-02-v2-SUMMARY.md`:

# Phase 1 Plan 02-v2: Lazy Graph Navigation Summary

**LazyCodeGraph with on-demand indexing, persistent caching, and LLM-based smart invalidation**

## Accomplishments

- LazyCodeGraph with zero startup cost (no eager indexing)
- JSON-based persistent cache in .auzoom/metadata/
- Content-hash validation for cache invalidation
- LLM-powered semantic diff analysis for summary updates
- Entry point discovery with background warming
- Import discovery without parsing
- 5x+ cache speedup over cold parse
- Comprehensive test suite proving lazy behavior

## Files Created/Modified

- `auzoom/src/auzoom/lazy_graph.py` - Complete lazy loading implementation
- `tests/test_lazy_graph.py` - Integration tests for lazy behavior
- `.auzoom/index.json` - File index with hashes and metadata
- `.auzoom/metadata/*.json` - Per-file cached parse results

## Decisions Made

- JSON over SQLite for simplicity and debuggability
- Content hash (SHA256, 8 chars) for cache keys
- LLM invalidation optional (degrades to hash-only without API key)
- Background warming non-blocking and optional (auto_warm flag)
- Import discovery during parse but deferred loading
- Cache structure: one JSON per file with content hash in filename

## Issues Encountered

- LLM invalidation requires ANTHROPIC_API_KEY env var
- Import resolution simplified for V1 (doesn't handle all edge cases)
- Background warming timing is heuristic (0.1s delay before start)
- File watching not implemented (manual invalidation on file change)

## Architecture Insights

**Why this matters**: Traditional code indexers (ctags, LSP servers) eagerly parse entire codebases, causing 10-60s startup delays for large projects. LazyCodeGraph starts in <100ms and parses only what's accessed, enabling instant navigation even for million-line codebases.

**Cache strategy**: Each file gets a JSON cache named `{path}_{hash}.json`. When file content changes, hash changes, old cache ignored, new parse triggered. LLM checks diff to decide if summary needs regeneration (avoids re-parse on comment-only changes).

**Memory model**: Only loaded files stay in RAM. Index keeps metadata (hashes, import lists) in memory for fast lookups. Full parse results cached to disk.

## Next Phase Readiness

Ready for 01-03-PLAN.md (MCP server with lazy graph integration)

**Critical update needed**: 01-03-PLAN.md should use LazyCodeGraph instead of eager-indexing CodeGraph, removing _index_project() call from AuZoomServer.__init__.

## Supersedes

This plan supersedes `01-02-PLAN.md` (eager CodeGraph). The eager implementation is preserved but not used in V1.
</output>
