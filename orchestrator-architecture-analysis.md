# Orchestrator Architecture: MCP Tools vs Direct Switching + AuZoom Integration

## The Core Question

Should other LLMs be exposed as MCP tools to Claude Code, or should an external orchestrator handle switching?

---

## Option A: LLMs as MCP Tools

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      CLAUDE CODE                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Available Tools:                                        â”‚â”‚
â”‚  â”‚  â€¢ auzoom_get_graph()     - Code navigation             â”‚â”‚
â”‚  â”‚  â€¢ call_haiku()           - Simple generation           â”‚â”‚
â”‚  â”‚  â€¢ call_qwen_local()      - Local model                 â”‚â”‚
â”‚  â”‚  â€¢ call_glm()             - Fast Cerebras               â”‚â”‚
â”‚  â”‚  â€¢ validate_with_sonnet() - Validation checkpoint       â”‚â”‚
â”‚  â”‚  â€¢ read_file(), write_file(), bash(), etc.             â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                              â”‚
â”‚  Claude Code decides: "This looks simple, I'll use Haiku"   â”‚
â”‚  â†’ tool_call: call_haiku(prompt="Write a sort function")    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pros
- Claude Code sees full context, makes informed decisions
- Can combine models mid-task (Haiku drafts, Sonnet reviews)
- Natural integration with existing tool ecosystem
- Claude Code can override routing when it knows better

### Cons
- Claude Code consumes tokens just to DECIDE which tool to call
- Every model switch requires Claude Code round-trip
- Claude Code's judgment may not be cost-optimal
- Slower: decision â†’ tool call â†’ wait â†’ process response â†’ next decision

---

## Option B: External Orchestrator (Pre-Router)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ORCHESTRATOR LAYER                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ 1. Receive task                                         â”‚â”‚
â”‚  â”‚ 2. Score complexity (rule-based, no LLM needed)         â”‚â”‚
â”‚  â”‚ 3. Route to appropriate model                           â”‚â”‚
â”‚  â”‚ 4. If simple (complexity â‰¤ 2): Haiku/Local handles ALL  â”‚â”‚
â”‚  â”‚ 5. If complex: Pass to Claude Code                      â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                  â–¼                  â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  LOCAL  â”‚      â”‚   HAIKU   â”‚      â”‚ CLAUDE    â”‚
   â”‚  Qwen   â”‚      â”‚           â”‚      â”‚  CODE     â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚ (Sonnet)  â”‚
                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pros
- Zero LLM tokens for routing decision
- Simple tasks never touch expensive models
- Faster: direct routing without deliberation
- Predictable costs based on complexity rules

### Cons
- Loses Claude Code's contextual judgment
- Rule-based routing can't adapt to nuance
- May misroute edge cases
- Another service to maintain

---

## Option C: Hybrid (RECOMMENDED)

**Key Insight**: Use BOTH. Orchestrator handles mechanical routing, Claude Code can override.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ORCHESTRATOR MCP SERVER                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚                    orchestrator_route()                         â”‚â”‚
â”‚  â”‚  - Scores complexity (rule-based)                               â”‚â”‚
â”‚  â”‚  - Returns: {model, reason, confidence}                         â”‚â”‚
â”‚  â”‚  - Does NOT execute, just recommends                            â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚                    orchestrator_execute()                       â”‚â”‚
â”‚  â”‚  - Takes: {model, prompt, context}                              â”‚â”‚
â”‚  â”‚  - Routes to specified model                                    â”‚â”‚
â”‚  â”‚  - Returns: {response, tokens_used, latency}                    â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚                    orchestrator_validate()                      â”‚â”‚
â”‚  â”‚  - Always uses Sonnet (input-heavy mode)                        â”‚â”‚
â”‚  â”‚  - Returns: {pass, issues, confidence, escalate}                â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           CLAUDE CODE                                â”‚
â”‚                                                                      â”‚
â”‚  Workflow:                                                          â”‚
â”‚  1. Get routing recommendation: orchestrator_route(task)            â”‚
â”‚  2. Accept or override: "Orchestrator says Haiku, but I see this    â”‚
â”‚     involves auth code, I'll use Sonnet"                            â”‚
â”‚  3. Execute: orchestrator_execute(model="sonnet", prompt=...)       â”‚
â”‚  4. Optionally validate: orchestrator_validate(output)              â”‚
â”‚                                                                      â”‚
â”‚  Claude Code DELEGATES generation but RETAINS judgment              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why This Works

1. **Cost Optimization**: Orchestrator's routing costs 0 LLM tokens
2. **Judgment Preserved**: Claude Code can override with full context
3. **Flexibility**: Can run in "auto" mode (trust orchestrator) or "manual" (always decide)
4. **Observability**: All routing decisions logged for learning

---

## AuZoom Integration: The Compound Effect

AuZoom reduces **input tokens** (what gets sent to models).
Hierarchical routing reduces **output cost** (which model generates).

**Combined effect is multiplicative, not additive.**

### Without AuZoom + Without Routing (Baseline)

```
Task: "Fix the login bug in auth/service.py"

Claude Code (Sonnet):
  Input:  auth/service.py (800 lines, 3,200 tokens)
        + auth/repository.py (400 lines, 1,600 tokens)
        + auth/models.py (300 lines, 1,200 tokens)
        + system prompt (500 tokens)
  = 6,500 input tokens Ã— $3/1M = $0.0195

  Output: Fix + explanation (800 tokens)
  = 800 output tokens Ã— $15/1M = $0.012

  Total: $0.0315 per task
```

### With AuZoom Only

```
Task: "Fix the login bug in auth/service.py"

Claude Code (Sonnet):
  Input:  auzoom skeleton (all auth/, 400 tokens)
        + auzoom summary (login function, 80 tokens)
        + auzoom full (login function only, 300 tokens)
        + system prompt (500 tokens)
  = 1,280 input tokens Ã— $3/1M = $0.00384

  Output: Fix + explanation (800 tokens)
  = 800 output tokens Ã— $15/1M = $0.012

  Total: $0.01584 per task (50% reduction)
```

### With Routing Only

```
Task: "Fix the login bug in auth/service.py"

Haiku generates fix (no AuZoom):
  Input:  6,500 tokens Ã— $0.25/1M = $0.001625
  Output: 800 tokens Ã— $1.25/1M = $0.001
  Subtotal: $0.002625

Sonnet validates (input-heavy):
  Input:  fix + context (2,000 tokens) Ã— $3/1M = $0.006
  Output: validation (100 tokens) Ã— $15/1M = $0.0015
  Subtotal: $0.0075

  Total: $0.010125 per task (68% reduction)
```

### With BOTH AuZoom + Routing (Compound Effect)

```
Task: "Fix the login bug in auth/service.py"

Step 1: Orchestrator routes to Haiku (0 tokens, rule-based)

Step 2: AuZoom navigation
  auzoom_get_graph(node="auth", level="skeleton", depth=2)
  â†’ 400 tokens, identifies login() needs fixing

Step 3: Haiku generates fix (with minimal context)
  Input:  skeleton (400) + login summary (80) + login full (300) = 780 tokens
  Ã— $0.25/1M = $0.000195
  Output: 500 tokens Ã— $1.25/1M = $0.000625
  Subtotal: $0.00082

Step 4: Sonnet validates (input-heavy, AuZoom-optimized context)
  Input:  fix (500) + relevant context via AuZoom (600) = 1,100 tokens
  Ã— $3/1M = $0.0033
  Output: 100 tokens Ã— $15/1M = $0.0015
  Subtotal: $0.0048

  Total: $0.00562 per task (82% reduction from baseline!)
```

### Cost Comparison Summary

| Approach | Cost/Task | Reduction |
|----------|-----------|-----------|
| Baseline (Sonnet, full files) | $0.0315 | - |
| AuZoom only | $0.0158 | 50% |
| Routing only | $0.0101 | 68% |
| **AuZoom + Routing** | **$0.0056** | **82%** |

---

## Integrated Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           MCP SERVER BUNDLE                              â”‚
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                         ðŸ± AuZoom                                  â”‚ â”‚
â”‚  â”‚  auzoom_get_graph(node, level, depth)                              â”‚ â”‚
â”‚  â”‚  auzoom_find(query, scope)                                         â”‚ â”‚
â”‚  â”‚  auzoom_get_dependencies(node, direction)                          â”‚ â”‚
â”‚  â”‚                                                                    â”‚ â”‚
â”‚  â”‚  Returns: Multi-resolution code view (skeleton/summary/full)       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                    â”‚                                     â”‚
â”‚                                    â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                    ðŸŽ­ Orchestrator                                 â”‚ â”‚
â”‚  â”‚  orchestrator_route(task, context) â†’ {model, confidence}           â”‚ â”‚
â”‚  â”‚  orchestrator_execute(model, prompt, auzoom_context) â†’ response    â”‚ â”‚
â”‚  â”‚  orchestrator_validate(output, context) â†’ {pass, issues}           â”‚ â”‚
â”‚  â”‚                                                                    â”‚ â”‚
â”‚  â”‚  Routing Rules:                                                    â”‚ â”‚
â”‚  â”‚  â€¢ complexity â‰¤ 2 â†’ local/qwen3-30b                               â”‚ â”‚
â”‚  â”‚  â€¢ complexity â‰¤ 4, code â†’ cerebras/glm-4.7                        â”‚ â”‚
â”‚  â”‚  â€¢ complexity â‰¤ 4, other â†’ haiku                                  â”‚ â”‚
â”‚  â”‚  â€¢ complexity â‰¤ 7 â†’ sonnet                                        â”‚ â”‚
â”‚  â”‚  â€¢ complexity > 7 â†’ opus                                          â”‚ â”‚
â”‚  â”‚  â€¢ validation checkpoint â†’ sonnet (max 100 tokens out)            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                    â”‚                                     â”‚
â”‚                                    â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                    ðŸ“Š Memory                                       â”‚ â”‚
â”‚  â”‚  memory_retrieve(query) â†’ relevant past experiences                â”‚ â”‚
â”‚  â”‚  memory_store(task, model, outcome) â†’ logged                       â”‚ â”‚
â”‚  â”‚  memory_get_profile(model) â†’ task affinities                       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           CLAUDE CODE                                    â”‚
â”‚                                                                          â”‚
â”‚  System prompt includes:                                                â”‚
â”‚  "You have access to AuZoom for efficient code navigation and          â”‚
â”‚   Orchestrator for model routing. ALWAYS use auzoom_get_graph          â”‚
â”‚   before reading full files. PREFER orchestrator_execute over          â”‚
â”‚   direct generation for delegatable tasks."                            â”‚
â”‚                                                                          â”‚
â”‚  Typical workflow:                                                      â”‚
â”‚  1. auzoom_get_graph(node="root", level="skeleton", depth=2)           â”‚
â”‚     â†’ Understand codebase structure (400 tokens)                       â”‚
â”‚  2. Identify relevant nodes from skeleton                              â”‚
â”‚  3. auzoom_get_graph(node="target", level="summary")                   â”‚
â”‚     â†’ Get signatures/docstrings (80 tokens)                            â”‚
â”‚  4. orchestrator_route(task_description)                               â”‚
â”‚     â†’ Get model recommendation                                         â”‚
â”‚  5. orchestrator_execute(model, prompt_with_auzoom_context)            â”‚
â”‚     â†’ Delegate to optimal model                                        â”‚
â”‚  6. If validation_required:                                            â”‚
â”‚     orchestrator_validate(output, auzoom_context)                      â”‚
â”‚     â†’ Sonnet reviews (input-heavy)                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## AuZoom-Aware Context Building

The orchestrator should build context using AuZoom levels based on the target model:

```python
class AuZoomContextBuilder:
    """Build optimal context for each model tier"""
    
    def build_context(self, task: str, target_model: str) -> str:
        if target_model in ["local/qwen3-30b", "haiku"]:
            # Minimal context for simple models
            return self._build_minimal_context(task)
        elif target_model in ["cerebras/glm-4.7", "sonnet"]:
            # Medium context with summaries
            return self._build_medium_context(task)
        else:  # opus
            # Full context for complex reasoning
            return self._build_full_context(task)
    
    def _build_minimal_context(self, task: str) -> str:
        """Skeleton + target function full only"""
        relevant_nodes = self.auzoom.find(task)
        
        context_parts = [
            # Skeleton for navigation
            self.auzoom.get_graph("root", level="skeleton", depth=2),
            # Full code only for the specific target
            self.auzoom.get_graph(relevant_nodes[0], level="full")
        ]
        return self._format(context_parts)
    
    def _build_medium_context(self, task: str) -> str:
        """Skeleton + summaries for dependencies + target full"""
        relevant = self.auzoom.find(task)
        deps = self.auzoom.get_dependencies(relevant[0], depth=1)
        
        context_parts = [
            self.auzoom.get_graph("root", level="skeleton", depth=2),
            # Summaries for dependencies (understand interfaces)
            *[self.auzoom.get_graph(d, level="summary") for d in deps],
            # Full for target
            self.auzoom.get_graph(relevant[0], level="full")
        ]
        return self._format(context_parts)
    
    def _build_full_context(self, task: str) -> str:
        """Full code for target and immediate dependencies"""
        relevant = self.auzoom.find(task)
        deps = self.auzoom.get_dependencies(relevant[0], depth=1)
        
        context_parts = [
            self.auzoom.get_graph("root", level="skeleton", depth=3),
            *[self.auzoom.get_graph(d, level="full") for d in deps[:3]],
            self.auzoom.get_graph(relevant[0], level="full")
        ]
        return self._format(context_parts)
```

---

## Model-Specific Prompt Templates

Different models need different prompt structures:

```python
PROMPT_TEMPLATES = {
    "local/qwen3-30b": """
<task>{task}</task>
<code_context>
{auzoom_skeleton}
</code_context>
<target_code>
{target_full}
</target_code>
Output only the modified code, no explanation.
""",
    
    "cerebras/glm-4.7": """
<task>{task}</task>
<navigation>
{auzoom_skeleton}
</navigation>
<dependencies>
{dependency_summaries}
</dependencies>
<modify>
{target_full}
</modify>
Think step by step. Output the fix.
""",
    
    "haiku": """
Task: {task}

Context (skeleton):
{auzoom_skeleton}

Code to modify:
{target_full}

Provide the corrected code.
""",
    
    "sonnet_validation": """
<context>{task_and_requirements}</context>
<code_structure>{auzoom_skeleton}</code_structure>
<output_to_validate>{generated_code}</output_to_validate>

Validate. JSON only (max 100 tokens):
{{"pass": bool, "issues": ["..."], "confidence": 0-1, "escalate": bool}}
"""
}
```

---

## Recommendation: Single MCP Server

Bundle everything into ONE MCP server with multiple tools:

```typescript
// claude_mcp_config.json
{
  "mcpServers": {
    "ai-orchestrator": {
      "command": "python",
      "args": ["-m", "ai_orchestrator.server"],
      "env": {
        "ANTHROPIC_API_KEY": "...",
        "CEREBRAS_API_KEY": "...",
        "OLLAMA_HOST": "http://localhost:11434",
        "QDRANT_HOST": "localhost:6333"
      }
    }
  }
}
```

Tools exposed:
```
auzoom_get_graph      - Multi-resolution code navigation
auzoom_find           - Semantic search in codebase
auzoom_dependencies   - Dependency analysis
orchestrator_route    - Get routing recommendation
orchestrator_execute  - Execute on specified model
orchestrator_validate - Input-heavy validation
memory_retrieve       - Get relevant past experiences
memory_store          - Log outcome for learning
```

This keeps Claude Code's tool list clean while providing full orchestration capability.
