---
phase: 01-audit-foundation-traceability
plan: 02
type: execute
---

<objective>
Create audit test infrastructure for systematic verification throughout the audit.

Purpose: Establish reusable harness, logging, and evidence collection framework that all subsequent audit phases will use.

Output: Audit infrastructure code and templates enabling consistent, evidence-based testing.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
.planning/phases/01-audit-foundation-traceability/01-02-SUMMARY.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-audit-foundation-traceability/01-01-SUMMARY.md

**Audit requirements (from PROJECT.md):**
- Evidence-based: Every finding documented with file references, line numbers, test results
- Non-destructive testing: Don't break working functionality
- Real API calls acceptable: Cost of thoroughness
- Documentation required: All findings with evidence and traceability

**Infrastructure needs:**
- Systematic test execution across 12 phases
- Evidence logging (inputs, outputs, errors, timing)
- Result aggregation and reporting
- Baseline comparison capabilities
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create audit harness with test execution framework</name>
  <files>audit/harness.py, audit/__init__.py, audit/models.py</files>
  <action>
    Create Python audit harness in `audit/` directory with:

    1. **Test execution framework** (harness.py):
       - `AuditTest` base class with: name, category, execute(), verify(), evidence()
       - `AuditRunner` class: register tests, run all, run by category
       - Error handling: catch exceptions, log stacktraces, continue execution
       - Timing: Record start/end times for each test
       - Result codes: PASS, FAIL, PARTIAL, SKIP

    2. **Evidence collection** (harness.py):
       - `Evidence` class: test_name, timestamp, type (file/api/measurement), data, metadata
       - Auto-capture: stdin/stdout, file diffs, API responses, timing
       - Storage: JSON Lines format in `audit/evidence/{test_name}_{timestamp}.jsonl`

    3. **Data models** (models.py):
       - `TestResult`: name, status, duration_ms, evidence_refs, error_msg
       - `AuditReport`: phase, tests_run, passed, failed, partial, evidence_dir
       - Pydantic v2 models for validation

    Design for extension: Subclass `AuditTest` for specific verification types (AuZoomTest, OrchestratorTest, IntegrationTest).

    Use existing project structure: Python 3.11+, Pydantic v2, pytest for test discovery if needed.
  </action>
  <verify>pytest audit/test_harness.py passes; harness.py imports without errors; AuditRunner can instantiate</verify>
  <done>Harness code complete, basic tests pass, ready for use in Phase 2+</done>
</task>

<task type="auto">
  <name>Task 2: Create audit logging and report templates</name>
  <files>audit/logger.py, audit/templates/test_report.md, audit/templates/evidence_log.jsonl, audit/README.md</files>
  <action>
    Create logging infrastructure:

    1. **Structured logger** (logger.py):
       - `AuditLogger` class wrapping Python logging
       - Formats: Console (human-readable), File (JSON structured)
       - Levels: DEBUG (all operations), INFO (test start/end), ERROR (failures)
       - Context injection: test_name, phase, timestamp auto-added
       - Output: `audit/logs/audit_{date}.log` and console

    2. **Test report template** (templates/test_report.md):
       - Markdown template for per-test reporting
       - Sections: Objective, Method, Evidence, Result, Findings, Severity
       - Example filled template showing usage
       - Placeholders: {{test_name}}, {{status}}, {{evidence_refs}}, {{findings}}

    3. **Evidence log format** (templates/evidence_log.jsonl):
       - JSON Lines schema documentation
       - Example entries for each evidence type (file, api, measurement, error)
       - Validation: JSON schema for evidence entries

    4. **Usage documentation** (README.md in audit/):
       - Quick start: How to write audit tests
       - Evidence collection: How to log evidence
       - Report generation: How to produce final reports
       - Examples: Complete test implementation with evidence
       - Directory structure explanation

    Follow AuZoom structural constraints: Keep files ≤250 lines, functions ≤50 lines.
  </action>
  <verify>audit/logger.py imports; templates exist and are valid markdown/JSON; README.md readable</verify>
  <done>Logging and templates complete, documented, ready for use</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `audit/` directory structure created
- [ ] Harness code passes basic tests
- [ ] Logger writes to file and console
- [ ] Templates valid and documented
- [ ] README.md explains usage clearly
</verification>

<success_criteria>

- All 2 tasks completed
- Audit infrastructure functional and documented
- Ready for use in Phases 2-11
- Follows AuZoom structural constraints
</success_criteria>

<output>
After completion, create `.planning/phases/01-audit-foundation-traceability/01-02-SUMMARY.md`:

# Phase 1 Plan 02: Audit Infrastructure Summary

**Created reusable audit harness with test execution, evidence logging, and report templates**

## Accomplishments

- Built Python audit harness with AuditTest base class and AuditRunner
- Implemented evidence collection framework (JSON Lines format)
- Created structured logger with console + file output
- Designed report templates (Markdown + JSON schema)
- Documented usage in audit/README.md

## Files Created/Modified

- `audit/harness.py` - Test execution framework
- `audit/models.py` - Pydantic data models
- `audit/logger.py` - Structured logging
- `audit/templates/test_report.md` - Report template
- `audit/templates/evidence_log.jsonl` - Evidence schema
- `audit/README.md` - Usage documentation

## Decisions Made

- Python-based infrastructure (matches AuZoom/Orchestrator stack)
- JSON Lines for evidence (append-only, streaming-friendly)
- Pydantic v2 models (validation + serialization)

## Issues Encountered

[Problems during development, or "None"]

## Next Step

Ready for 01-03-PLAN.md (baseline metrics capture)
</output>
