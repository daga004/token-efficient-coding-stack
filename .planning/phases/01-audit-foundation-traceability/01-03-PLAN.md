---
phase: 01-audit-foundation-traceability
plan: 03
type: execute
---

<objective>
Capture baseline state of all systems before audit modifications begin.

Purpose: Establish "before" snapshot enabling comparison after audit changes. Ensures we can measure impact of fixes and verify claims against starting state.

Output: Baseline report with current metrics, git references, and validation results frozen in time.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
.planning/phases/01-audit-foundation-traceability/01-03-SUMMARY.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-audit-foundation-traceability/01-01-SUMMARY.md
@.planning/phases/01-audit-foundation-traceability/01-02-SUMMARY.md
@VALIDATION-SUMMARY.md
@README.md

**Baseline requirements:**
- Current validation metrics (79.5% cost savings, 100%/67% quality)
- Git state (commit hashes for reproducibility)
- File statistics (AuZoom violations, line counts)
- MCP server status (running, accessible, tool availability)
- Claimed vs actual metrics comparison

**Why baseline matters:**
- Audit will modify code to fix gaps
- Need "before" state to measure improvement
- Verify current claims are reproducible
- Establish regression prevention baseline
</context>

<tasks>

<task type="auto">
  <name>Task 1: Capture current system state and validation metrics</name>
  <files>audit/baseline/BASELINE-REPORT.md, audit/baseline/metrics.json</files>
  <action>
    Create comprehensive baseline report capturing current state:

    1. **Git state**:
       - Current commit: `git rev-parse HEAD`
       - Branch: `git branch --show-current`
       - Dirty state: `git status --porcelain` (if any uncommitted changes)
       - Latest commit: `git log -1 --pretty=format:"%H %s %ai"`

    2. **Validation metrics** (from VALIDATION-SUMMARY.md):
       - Parse task-by-task table (lines 13-31)
       - Extract: Total tokens baseline vs optimized
       - Extract: Total cost baseline vs optimized
       - Extract: Success rates (10/10 simple, 3/5 challenging)
       - Calculate: Actual percentage savings vs claimed (79.5%)

    3. **Codebase statistics**:
       - Run `auzoom_validate` if available (structural compliance)
       - Count Python files: `find . -name "*.py" -not -path "*/__pycache__/*" | wc -l`
       - LOC counts: `cloc auzoom/ orchestrator/ --json`
       - Function counts, max function length, max file length

    4. **MCP server status**:
       - AuZoom MCP: Check if tools defined in auzoom/src/auzoom/mcp/server.py
       - Orchestrator MCP: Check if tools defined in orchestrator/src/orchestrator/mcp/server.py
       - Tool counts: auzoom_read, auzoom_find, auzoom_get_dependencies, auzoom_stats, auzoom_validate
       - Tool counts: orchestrator_route, orchestrator_execute (if exists)

    Write structured JSON (metrics.json) + human-readable report (BASELINE-REPORT.md).

    Use audit infrastructure from 01-02: `AuditLogger` for logging, evidence collection for artifacts.
  </action>
  <verify>audit/baseline/ directory exists; BASELINE-REPORT.md readable; metrics.json valid JSON; git commit hash matches current HEAD</verify>
  <done>Complete baseline captured with git state, validation metrics, codebase stats, MCP status</done>
</task>

<task type="auto">
  <name>Task 2: Create baseline comparison framework for post-audit verification</name>
  <files>audit/baseline_compare.py, audit/baseline/BASELINE-README.md</files>
  <action>
    Create comparison utility for measuring audit impact:

    1. **Comparison script** (baseline_compare.py):
       - `load_baseline(path)` - Loads metrics.json from baseline snapshot
       - `capture_current()` - Re-runs Task 1 measurements on current state
       - `compare(baseline, current)` - Computes deltas and improvements
       - Output: Comparison report showing before/after for all metrics
       - Highlighting: Green for improvements, red for regressions, yellow for no change

    2. **Comparison categories**:
       - Validation metrics: Token/cost savings delta
       - Structural compliance: Violations fixed count
       - Test coverage: New tests added
       - Git commits: Number of audit fix commits

    3. **Usage documentation** (BASELINE-README.md):
       - How baseline was captured (date, commit, method)
       - How to use baseline_compare.py after audit
       - Expected improvements from audit (based on PROJECT.md goals)
       - Regression prevention: What should NOT change

    Design baseline_compare.py to be runnable at audit end (Phase 12) for final verification.

    Follow AuZoom constraints: Functions ≤50 lines, module ≤250 lines.
  </action>
  <verify>baseline_compare.py imports; has compare() function; BASELINE-README.md documents usage; smoke test with current=baseline shows 0 deltas</verify>
  <done>Comparison framework ready for post-audit verification in Phase 12</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] Baseline report captures all required metrics
- [ ] Git commit hash recorded (reproducibility)
- [ ] Validation metrics match VALIDATION-SUMMARY.md claims
- [ ] Comparison framework functional
- [ ] Phase 1 complete (all 3 plans done)
</verification>

<success_criteria>

- All 2 tasks completed
- Baseline snapshot frozen and documented
- Comparison framework ready for Phase 12
- **Phase 1 complete**: Foundation and traceability established
</success_criteria>

<output>
After completion, create `.planning/phases/01-audit-foundation-traceability/01-03-SUMMARY.md`:

# Phase 1 Plan 03: Baseline Metrics Capture Summary

**Captured comprehensive baseline of current system state at commit [hash] for audit comparison**

## Accomplishments

- Recorded git state (commit, branch, status) for reproducibility
- Extracted validation metrics from VALIDATION-SUMMARY.md (79.5% claimed)
- Captured codebase statistics (LOC, file counts, AuZoom violations)
- Documented MCP server tool availability
- Created baseline comparison framework for post-audit verification
- **Phase 1 complete**: All foundation and traceability tasks done

## Files Created/Modified

- `audit/baseline/BASELINE-REPORT.md` - Human-readable baseline state
- `audit/baseline/metrics.json` - Structured baseline data
- `audit/baseline_compare.py` - Comparison utility for Phase 12
- `audit/baseline/BASELINE-README.md` - Usage documentation

## Decisions Made

- Baseline captured at commit [hash] - all audit phases will reference this
- JSON format for programmatic comparison
- Smoke tested comparison framework (current=baseline → 0 deltas)

## Issues Encountered

[Problems during capture, or "None"]

## Next Phase Readiness

**Phase 1 complete**. Ready for Phase 2: AuZoom Core Verification.

All audit infrastructure in place:
- ✅ WISHLIST-COMPLIANCE.md traceability document
- ✅ Audit harness and evidence collection
- ✅ Baseline snapshot for comparison

No blockers for Phase 2.
</output>
