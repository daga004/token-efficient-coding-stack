---
phase: 06.5-progressive-traversal-validation
plan: 01
type: execute
---

<objective>
Execute 10 representative tasks with real Claude Code Task tool to measure interaction patterns, depth progression, and conversation overhead in progressive traversal workflow.

Purpose: Validate that agents naturally traverse progressively (skeleton → summary → full) rather than immediately requesting full depth, and measure actual token consumption vs baseline.
Output: Interaction pattern analysis report with average depth, follow-up frequency, net token savings, and quality assessment.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06.5-progressive-traversal-validation/CONTEXT.md

# Phase 5 findings
@.planning/phases/05-validation-metrics-reexecution/PHASE-05-SYNTHESIS.md

# User intent clarification
CONTEXT.md documents progressive on-demand traversal as core feature:
- Initial request → skeleton (graph overview)
- Follow-up questions → summary (targeted detail)
- Implementation needs → full read

**Critical validation gap**: Phase 5 validated static level selection, not progressive interactive traversal.
</context>

<tasks>

<task type="auto">
  <name>Task 1: Define 10 representative tasks covering traversal patterns</name>
  <files>audit/tests/test_progressive_traversal.py</files>
  <action>Create test suite with 10 tasks covering different depth patterns:

**Shallow tasks** (skeleton sufficient - 2 tasks):
1. "List all public functions in auzoom/src/auzoom/server.py"
   - Expected depth: 1.0 (skeleton only)
   - Success: Function list accurate, no deeper reads needed
2. "Show module structure of orchestrator/ directory"
   - Expected depth: 1.0 (skeleton for each file)
   - Success: Structure overview complete

**Medium depth tasks** (summary sufficient - 3 tasks):
3. "What does auzoom_read function do in server.py?"
   - Expected depth: 2.0 (skeleton to locate → summary for docstring)
   - Success: Accurate description without full implementation
4. "Find all MCP tools defined in server.py"
   - Expected depth: 2.0 (summary level shows tool definitions)
   - Success: Complete tool list with signatures
5. "Explain the caching strategy in auzoom/src/auzoom/cache.py"
   - Expected depth: 2.0 (summary includes strategy description)
   - Success: Correct strategy explanation

**Deep tasks** (full read required - 3 tasks):
6. "Fix potential bug in token counting (if any) in auzoom/src/auzoom/parser.py"
   - Expected depth: 3.0 (need full implementation to find bugs)
   - Success: Bug identified and fixed, or confirmed no bugs
7. "Add error handling to auzoom_validate function"
   - Expected depth: 3.0 (need full code to add logic)
   - Success: Error handling added correctly
8. "Optimize the dependency resolution algorithm in auzoom/src/auzoom/graph.py"
   - Expected depth: 3.0 (need full algorithm to optimize)
   - Success: Optimization implemented, performance improved

**Graph navigation tasks** (dependency traversal - 2 tasks):
9. "Find all functions that call validate_path"
   - Expected depth: 1.5 (skeleton + dependency graph, no full reads)
   - Success: Complete caller list using dependency traversal
10. "Show the call chain from auzoom_read to Python AST parsing"
    - Expected depth: 2.0 (skeleton + dependencies, maybe some summaries)
    - Success: Accurate call chain without full file reads

For each task:
- Define clear success criteria (what output is correct?)
- Specify expected depth (1.0 = skeleton, 2.0 = summary, 3.0 = full)
- Note baseline token count (if known from Phase 5)
- Include quality validation method

Store in pytest framework with:
```python
@pytest.mark.parametrize("task_id,description,expected_depth,success_criteria", [
    (1, "List all public functions...", 1.0, "Function list matches actual"),
    ...
])
def test_progressive_traversal_task(task_id, description, expected_depth, success_criteria):
    # Task definition stored here
    pass
```
</action>
  <verify>cat audit/tests/test_progressive_traversal.py shows 10 tasks with expected depths and success criteria</verify>
  <done>10 tasks defined covering shallow (2), medium (3), deep (3), and graph navigation (2) patterns</done>
</task>

<task type="auto">
  <name>Task 2: Execute tasks with real Claude Code Task tool and log interactions</name>
  <files>audit/progressive_executor.py, audit/evidence/progressive_traversal_*.jsonl</files>
  <action>Create executor that spawns Task agents with auzoom MCP:

```python
class ProgressiveTraversalExecutor:
    def execute_task(self, task_id, description):
        # Spawn Claude Code Task agent
        # Agent has access to auzoom_read MCP
        # Log each auzoom_read call:
        #   - Timestamp
        #   - File path
        #   - Level requested (skeleton/summary/full)
        #   - Tokens consumed (from API usage)
        #   - Agent's reasoning (why this depth?)

        # Track depth progression:
        # 1st read: level=skeleton → depth 1.0
        # 2nd read same file: level=summary → depth 2.0
        # 3rd read same file: level=full → depth 3.0

        # Track conversation overhead:
        # - Agent's internal reasoning tokens
        # - Follow-up requests to MCP
        # - Total tokens vs baseline (direct Read tool)

        # Measure:
        result = {
            "task_id": task_id,
            "description": description,
            "interactions": [
                {"file": "...", "level": "skeleton", "tokens": 150, "reasoning": "..."},
                {"file": "...", "level": "summary", "tokens": 1125, "reasoning": "..."},
            ],
            "final_depth": 2.0,  # Max depth reached for primary file
            "total_tokens": 1275,
            "conversation_overhead": 50,  # Agent reasoning between reads
            "baseline_tokens": 450,  # What Read tool would use
            "net_savings": -875,  # Negative if progressive used more
            "quality": "correct",  # Validate output matches success criteria
        }
        return result
```

**Execution approach**:
1. Use Task tool to spawn agent for each of 10 tasks
2. Agent has auzoom MCP server available
3. Log every auzoom_read call (file, level, tokens)
4. Calculate metrics after task completes
5. Validate quality against success criteria

**Important**: This is REAL execution, not simulation. We need actual agent behavior.

Write results to: `audit/evidence/progressive_traversal_20260113_*.jsonl`
</action>
  <verify>10 tasks executed, evidence file has 10 entries with depth progression and token measurements</verify>
  <done>Progressive executor complete, 10 tasks executed with real Task tool, interaction patterns logged</done>
</task>

<task type="auto">
  <name>Task 3: Analyze interaction patterns and calculate metrics</name>
  <files>audit/reports/06.5-01-interaction-patterns.md</files>
  <action>Analyze evidence from 10 task executions:

## 1. Depth Distribution Analysis

For each task:
- What was the final depth? (1.0, 2.0, or 3.0)
- Did it match expected depth?
- If not, why? (agent went deeper/shallower than needed)

**Calculate**:
- Average depth across all tasks
- Distribution: X% stayed at skeleton, Y% went to summary, Z% needed full
- Comparison to expectations

**Success criteria**:
- Average depth < 2.0 (most stay shallow/medium)
- ≥60% of tasks don't require full read
- Depth selection reasonably accurate (within ±0.5 of expected)

---

## 2. Token Consumption Analysis

For each task:
- Progressive tokens: Sum(all auzoom_reads) + conversation overhead
- Baseline tokens: Single Read tool call (from Phase 5 or measure now)
- Net savings: (baseline - progressive) / baseline

**Calculate**:
- Average net savings across 10 tasks
- Which task types save tokens? (shallow, medium, deep, graph)
- Which task types add overhead?

**Success criteria**:
- Net savings ≥20% on average
- OR: Net savings ≥0% with qualitative UX benefit documented

---

## 3. Conversation Overhead Analysis

**Measure**:
- Average overhead per follow-up read (agent reasoning between calls)
- Total overhead as % of total tokens
- Is overhead significant enough to negate savings?

**Example**:
```
Task 3: "What does auzoom_read do?"
- 1st read: skeleton (150 tokens) → agent sees function signature
- Overhead: 50 tokens (agent decides "need docstring")
- 2nd read: summary (1,125 tokens) → agent gets docstring
- Total: 150 + 50 + 1,125 = 1,325 tokens
- Baseline: 450 tokens (full read)
- Net: -875 tokens (195% worse)
```

---

## 4. Quality Assessment

For each task:
- Did agent produce correct output?
- Quality score: 100% (perfect), 75% (minor issues), 50% (major issues), 0% (failed)
- Did progressive depth cause quality loss? (missed info from staying shallow)

**Success criteria**:
- Quality ≥95% average (no degradation from progressive approach)
- No false negatives (stayed shallow when needed depth)

---

## 5. Overall Verdict

Based on metrics:
- Does progressive traversal save tokens? (≥20% net savings)
- Do agents naturally traverse progressively? (average depth < 2.0)
- Is quality maintained? (≥95%)
- What's the recommendation? (Use progressive, use static, use hybrid)

**Recommendation framework**:
- If net savings ≥20% AND quality ≥95%: **Progressive traversal validated** ✅
- If net savings 0-20% AND quality ≥95%: **Neutral** (UX benefit may justify)
- If net savings <0% (overhead exceeds savings): **Progressive not beneficial** ❌
  - Recommendation: Use static level selection or hybrid (progressive for large files only)

Include detailed breakdown by task type (shallow, medium, deep, graph).
</action>
  <verify>cat audit/reports/06.5-01-interaction-patterns.md shows comprehensive analysis with verdict on progressive traversal value</verify>
  <done>Interaction pattern analysis complete, verdict on progressive traversal value delivered</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] 10 tasks defined covering all traversal patterns (shallow, medium, deep, graph)
- [ ] All 10 tasks executed with real Task tool (not simulation)
- [ ] Interaction patterns logged (depth progression, token consumption)
- [ ] Metrics calculated (average depth, net savings, quality)
- [ ] Verdict delivered on progressive traversal value
</verification>

<success_criteria>
- All 10 tasks executed with real agents
- Interaction patterns logged with actual token measurements
- Average depth calculated (target: < 2.0)
- Net token savings calculated (target: ≥20% or ≥0% with UX benefit)
- Quality maintained (target: ≥95%)
- Clear verdict on whether progressive traversal delivers value
</success_criteria>

<output>
After completion, create `06.5-01-SUMMARY.md`:

# Phase 6.5 Plan 01: Interaction Pattern Analysis Summary

**[One-liner about whether agents naturally traverse progressively and token savings achieved]**

## Accomplishments

- 10 representative tasks defined and executed with real Task tool
- Interaction patterns logged (depth progression, conversation overhead)
- Average depth calculated: [X] (target: < 2.0)
- Net token savings: [X]% (target: ≥20%)
- Quality maintained: [X]% (target: ≥95%)

## Key Findings

**Depth Distribution**:
- [X]% stayed at skeleton (shallow tasks)
- [Y]% went to summary (medium depth)
- [Z]% needed full read (deep tasks)
- Average depth: [X] (expected: 1.8-2.0)

**Token Efficiency**:
- Net savings: [X]%
- Tasks with positive savings: [N]/10
- Tasks with overhead: [N]/10
- Conversation overhead: [X]% of total tokens

**Quality**:
- Average quality: [X]%
- Tasks with 100% quality: [N]/10
- Quality degradation from progressive approach: [X] instances

## Verdict

Progressive traversal: **VALIDATED / NOT BENEFICIAL / NEEDS REFINEMENT**

[Explanation based on metrics]

## Recommendations

[Based on findings - use progressive, static, or hybrid approach]
</output>
