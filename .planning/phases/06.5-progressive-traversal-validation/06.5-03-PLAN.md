---
phase: 06.5-progressive-traversal-validation
plan: 03
type: execute
---

<objective>
Validate that dependency graph-guided navigation reduces file reads by ≥30% vs blind grep-based search while maintaining 100% quality (no missed dependencies).

Purpose: Measure the specific benefit of graph-based navigation (auzoom_find + auzoom_get_dependencies) vs traditional search approaches, isolating this from progressive depth benefits.
Output: Graph navigation efficiency report with file read reduction, token savings, quality verification, and recommendations for multi-file tasks.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06.5-progressive-traversal-validation/CONTEXT.md

# Prior plan results
@.planning/phases/06.5-progressive-traversal-validation/06.5-01-SUMMARY.md
@.planning/phases/06.5-progressive-traversal-validation/06.5-02-SUMMARY.md

# Phase 2 validation
@.planning/phases/02-progressive-disclosure-validation/02-02-SUMMARY.md
(Dependency tracking validated at 100% precision/recall)

**This plan isolates**: Graph navigation benefit (fewer file reads) from progressive depth benefit (token savings per file)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Define multi-file and dependency tasks for graph validation</name>
  <files>audit/tests/test_graph_navigation.py</files>
  <action>Create test suite with 8 tasks specifically testing graph navigation:

**Direct dependency tasks** (find callers/callees - 3 tasks):
1. "Find all functions that call validate_path in auzoom/src/auzoom/"
   - Ground truth: [list known callers from codebase analysis]
   - Graph approach: auzoom_find("validate_path") → auzoom_get_dependencies(reverse=true)
   - Baseline approach: grep "validate_path" → read matching files → manual inspection
   - Expected: Graph reads 2-3 files, baseline reads 5-8 files

2. "Find all dependencies of auzoom_read function (what it calls)"
   - Ground truth: [list known dependencies]
   - Graph approach: auzoom_get_dependencies(node_id="auzoom_read", depth=1)
   - Baseline: Read auzoom_read → manually trace each function call → read those files
   - Expected: Graph reads 4-5 files, baseline reads 8-12 files

3. "Show the call chain from server.handle_request to Python AST parser"
   - Ground truth: [known call chain]
   - Graph approach: auzoom_get_dependencies with path finding
   - Baseline: Manual traversal reading multiple files
   - Expected: Graph reads 3-4 files, baseline reads 6-10 files

**Circular dependency tasks** (2 tasks):
4. "Identify any circular imports in auzoom/src/auzoom/"
   - Ground truth: [known circular dependencies or "none"]
   - Graph approach: auzoom_get_dependencies(depth=3) → detect cycles
   - Baseline: Read all files, trace imports manually
   - Expected: Graph reads 0 files (just graph traversal), baseline reads 15-20 files

5. "Fix circular import between scorer.py and executor.py (if exists)"
   - Graph approach: Identify cycle via graph → read only involved files
   - Baseline: Read all potentially related files to understand structure
   - Expected: Graph reads 2 files, baseline reads 8-12 files

**Refactoring tasks** (module rename/move - 2 tasks):
6. "Rename module cache.py to caching.py and update all imports"
   - Ground truth: [list all files importing cache.py]
   - Graph approach: auzoom_get_dependencies(node_id="cache.py", reverse=true) → edit importers
   - Baseline: grep "import cache" → read all matches → update
   - Expected: Graph identifies 6-8 importers, baseline reads 12-15 files checking false positives

7. "Extract validation logic from server.py into new validate.py module"
   - Graph approach: Identify validation functions → auzoom_get_dependencies → move with deps
   - Baseline: Manual code review → identify functions → trace dependencies by reading
   - Expected: Graph reads 3-4 files, baseline reads 8-10 files

**Cross-module analysis task** (1 task):
8. "Analyze the data flow from MCP request to final response"
   - Graph approach: auzoom_get_dependencies(depth=5) → trace request path
   - Baseline: Read multiple files following execution path manually
   - Expected: Graph reads 5-6 files, baseline reads 12-18 files

For each task:
- **Ground truth**: What files are actually relevant? (pre-compute from codebase)
- **Graph approach**: What auzoom tools are used?
- **Baseline approach**: What traditional tools are used? (grep, find, Read)
- **Expected file counts**: Graph X files, baseline Y files
- **Success criteria**: Graph finds 100% of relevant files, reads ≤70% as many as baseline

Store in pytest with file count expectations.
</action>
  <verify>cat audit/tests/test_graph_navigation.py shows 8 tasks with ground truth and expected file counts</verify>
  <done>8 graph navigation tasks defined with ground truth and baseline comparisons</done>
</task>

<task type="auto">
  <name>Task 2: Execute tasks with both approaches and measure file reads</name>
  <files>audit/graph_executor.py, audit/evidence/graph_navigation_*.jsonl</files>
  <action>Execute all 8 tasks with TWO approaches:

## Approach A: Graph Navigation (auzoom tools)

```python
class GraphNavigationExecutor:
    def execute_task(self, task_id, description):
        # Spawn Task agent with auzoom MCP
        # Agent uses auzoom_find + auzoom_get_dependencies
        # Log:
        #   - auzoom_find calls (file pattern matching)
        #   - auzoom_get_dependencies calls (graph traversal)
        #   - auzoom_read calls (actual file reads)

        result = {
            "task_id": task_id,
            "approach": "graph",
            "files_read": ["file1.py", "file2.py"],  # From auzoom_read logs
            "file_count": 2,
            "tokens_consumed": 1200,
            "graph_queries": 3,  # find + dependencies calls
            "quality": "correct",  # Matches ground truth?
        }
        return result
```

## Approach B: Baseline (grep + Read)

```python
class BaselineSearchExecutor:
    def execute_task(self, task_id, description):
        # Spawn Task agent with traditional tools only
        # Agent uses Grep + Read (no auzoom)
        # Log:
        #   - Grep searches
        #   - Read calls (file reads)
        #   - Manual dependency tracing

        result = {
            "task_id": task_id,
            "approach": "baseline",
            "files_read": ["file1.py", "file2.py", "file3.py", "file4.py"],
            "file_count": 4,
            "tokens_consumed": 2400,
            "grep_queries": 5,
            "quality": "correct",
        }
        return result
```

**Execution**:
1. For each of 8 tasks:
   - Execute with graph navigation (Approach A)
   - Execute with baseline search (Approach B)
2. Log all file reads for both approaches
3. Compare file counts: How many fewer files did graph approach read?
4. Validate quality: Did both find the correct answer?

**Critical**: Don't let baseline approach cheat by knowing ground truth. Agent should naturally explore via grep/read.

Write results to: `audit/evidence/graph_navigation_20260113_*.jsonl`
</action>
  <verify>16 executions complete (8 tasks × 2 approaches), evidence logged with file counts and quality</verify>
  <done>All 8 tasks executed with both graph and baseline approaches, file reads measured</done>
</task>

<task type="auto">
  <name>Task 3: Calculate file read reduction and generate efficiency report</name>
  <files>audit/reports/06.5-03-graph-navigation.md</files>
  <action>Analyze graph navigation efficiency:

# Graph Navigation Efficiency Report

## Executive Summary

**Overall verdict**: Graph navigation reduces file reads by [X]% vs baseline (target: ≥30%)

**Key metrics**:
- Average file reduction: [X]%
- Token savings: [Y]%
- Quality parity: [Z]% (target: 100%)

---

## 1. File Read Reduction Analysis

### Task-by-Task Comparison

| Task | Graph Files | Baseline Files | Reduction | Verdict |
|------|-------------|----------------|-----------|---------|
| 1 (callers) | 3 | 7 | **57%** | ✅ WIN |
| 2 (deps) | 5 | 11 | **45%** | ✅ WIN |
| 3 (chain) | 4 | 9 | **56%** | ✅ WIN |
| 4 (circular) | 0 | 18 | **100%** | ✅ WIN |
| 5 (fix) | 2 | 10 | **80%** | ✅ WIN |
| 6 (rename) | 8 | 14 | **43%** | ✅ WIN |
| 7 (extract) | 4 | 9 | **56%** | ✅ WIN |
| 8 (flow) | 6 | 15 | **60%** | ✅ WIN |

**Average reduction**: [X]%
**Win rate**: [Y]/8 tasks (target: 100%)

### By Task Type

**Dependency finding** (tasks 1-3):
- Graph: [X] avg files
- Baseline: [Y] avg files
- Reduction: [Z]%

**Circular dependency** (tasks 4-5):
- Graph: [X] avg files
- Baseline: [Y] avg files
- Reduction: [Z]%

**Refactoring** (tasks 6-7):
- Graph: [X] avg files
- Baseline: [Y] avg files
- Reduction: [Z]%

**Cross-module** (task 8):
- Graph: [X] files
- Baseline: [Y] files
- Reduction: [Z]%

---

## 2. Token Savings from Reduced Reads

Fewer files read = fewer tokens consumed.

**Calculation**:
```
For each task:
  graph_tokens = graph_files × avg_tokens_per_file
  baseline_tokens = baseline_files × avg_tokens_per_file
  savings = (baseline - graph) / baseline × 100%
```

**Assumptions**:
- Skeleton read: 150 tokens/file
- Full read: 450 tokens/file
- Use appropriate level based on task needs

**Results**:
- Average token savings: [X]%
- Total tokens saved (8 tasks): [Y]
- Savings consistent with file reduction? YES / NO

---

## 3. Quality Verification

For each task:
- Did graph approach find all relevant files? (compare to ground truth)
- Did graph approach miss any dependencies? (false negatives)
- Did baseline approach find all relevant files?

**Graph navigation quality**:
- [N]/8 tasks found 100% of relevant files
- [N] tasks had false negatives (missed files)
- [N] tasks had false positives (read irrelevant files)

**Baseline quality**:
- [N]/8 tasks found 100% of relevant files
- [N] tasks missed dependencies (explored incorrectly)

**Quality parity**: Graph = baseline quality? YES / NO

**Critical finding**: If graph MISSES dependencies, it's a failure even if file count is lower.

---

## 4. Graph Query Efficiency

How many graph queries did agents use?
- auzoom_find: [X] avg calls per task
- auzoom_get_dependencies: [Y] avg calls per task
- Total graph operations: [Z]

**Graph overhead**:
- Are graph queries expensive? (tokens consumed)
- Do multiple queries negate savings?

**Verdict**: Graph query overhead [IS / IS NOT] significant compared to file read savings.

---

## 5. Failure Analysis

**Tasks where graph didn't save ≥30%**:
- Task [N]: [reason - e.g., small codebase, few dependencies]
- Task [N]: [reason]

**Tasks where graph missed dependencies**:
- Task [N]: [what was missed, why]
- Root cause: [graph incomplete / agent didn't query correctly]

---

## 6. Recommendations

**When graph navigation excels**:
- Multi-file refactoring (rename, extract, move)
- Dependency analysis (callers, callees, chains)
- Circular dependency detection
- Large codebases (more files = more savings)

**When graph navigation is neutral**:
- Single-file tasks
- Small codebases (<10 files)
- Tasks requiring full context anyway

**When graph navigation might fail**:
- Dynamic imports (not captured in static graph)
- Cross-language dependencies (if not supported)
- Complex control flow (reflection, eval, etc.)

---

## 7. Overall Verdict

**Based on metrics**:
- File read reduction: [X]% (target: ≥30%)
- Token savings: [Y]% (target: ≥40%)
- Quality parity: [Z]% (target: 100%)

**Verdict**: **GRAPH NAVIGATION VALIDATED** ✅ / **NEEDS IMPROVEMENT** ⚠️ / **NOT BENEFICIAL** ❌

**Specific findings**:
1. Graph reduces file reads by [X]% on multi-file tasks
2. Token savings of [Y]% correlate with file reduction
3. Quality maintained at [Z]% (no significant false negatives)
4. Recommendation: [Always use graph / Use for tasks X,Y,Z / Needs refinement]

---

## 8. V1 Certification Impact

**Can V1 claim "dependency graphs reduce file reads by 30%+"?**
- [YES / NO / YES with caveats]

**Evidence**:
- [Summary of findings supporting claim]

**Required documentation updates**:
- [Any changes needed based on validation]

**Implementation recommendations**:
- [When to automatically use graph vs blind search]
</action>
  <verify>cat audit/reports/06.5-03-graph-navigation.md shows comprehensive efficiency analysis with verdict</verify>
  <done>Graph navigation efficiency report complete with file reduction metrics and recommendations</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] 8 graph-focused tasks defined with ground truth
- [ ] All tasks executed with both graph and baseline approaches
- [ ] File read counts measured for both approaches
- [ ] Average reduction calculated (target: ≥30%)
- [ ] Quality verified (no missed dependencies)
- [ ] Recommendations provided for when to use graph navigation
</verification>

<success_criteria>
- File read reduction ≥30% on average
- Token savings ≥40% from reduced reads
- Quality maintained at 100% (no false negatives)
- Clear recommendations on graph navigation applicability
- V1 certification impact assessed
</success_criteria>

<output>
After completion, create `06.5-03-SUMMARY.md`:

# Phase 6.5 Plan 03: Graph Navigation Efficiency Summary

**[One-liner about file read reduction percentage and graph navigation value]**

## Accomplishments

- 8 multi-file and dependency tasks defined and executed
- Graph navigation: [X] avg files read
- Baseline search: [Y] avg files read
- File read reduction: [Z]% (target: ≥30%)
- Token savings: [W]% (target: ≥40%)
- Quality parity: 100% (no missed dependencies)

## Key Findings

**File Read Efficiency**:
- Average reduction: [X]%
- Best case: Task [N] saved [Y]% files
- Worst case: Task [N] saved [Z]% files
- Win rate: [W]/8 tasks achieved ≥30% reduction

**Quality Verification**:
- [N]/8 tasks found 100% of relevant files
- [N] false negatives (missed dependencies): [NONE / list]
- Graph quality = baseline quality: [YES / NO]

**Token Savings**:
- Correlates with file reduction: [YES / NO]
- Average savings: [X]%

## Verdict

Graph navigation: **VALIDATED / NEEDS IMPROVEMENT / NOT BENEFICIAL**

[Explanation based on metrics]

## Recommendations

**Use graph navigation for**:
- Multi-file refactoring
- Dependency analysis
- Circular import detection
- Large codebases (>20 files)

**Use baseline for**:
- Single-file tasks
- Small codebases
- [Other cases]

## V1 Impact

[Can V1 claim 30%+ file reduction? What evidence supports this?]
</output>
