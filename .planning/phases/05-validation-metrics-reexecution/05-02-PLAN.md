---
phase: 05-validation-metrics-reexecution
plan: 02
type: execute
---

<objective>
Re-execute 15 challenging validation tasks with real Claude API calls to verify quality claims (67% success rate).

Purpose: Validate claimed quality metrics (100% simple, 67% challenging) using actual API execution instead of theoretical assessment.
Output: Real API execution results for challenging tasks, quality scoring evidence, success rate verification.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Prior work
@.planning/phases/05-validation-metrics-reexecution/05-01-SUMMARY.md

# Test infrastructure
@audit/real_api_executor.py
@audit/harness.py

# Validation definitions
@.planning/phases/03-integration-validation/TEST-SUITE.md
@VALIDATION-SUMMARY.md

# For reference - what was claimed
@.planning/phases/03-integration-validation/CHALLENGING-TEST-RESULTS.md

**Key context from STATE.md:**
- Claimed: 67% success rate on challenging tasks (3 fully working, 2 partial out of 5 tested)
- Claimed: Task 13 (input sanitization) had 0% - security failure
- Claimed: Task 7 (error handling) had 60% - missing edge cases
- Claimed: Task 9 (integration test) had 75% - wrong assertion
- Need to verify: Was this based on real execution or theoretical assessment?

**From VALIDATION-SUMMARY.md:**
- 15 challenging tasks defined but only 5 actually executed in Phase 3
- Tasks 11, 6, 9, 7, 13 were tested
- Remaining 10 tasks were never executed (just defined)
- This plan should execute ALL 15, not just the 5 from Phase 3
</context>

<tasks>

<task type="auto">
  <name>Task 1: Execute all 15 challenging tasks with real Claude API</name>
  <files>audit/tests/test_challenging_validation.py</files>
  <action>Create comprehensive test suite for all 15 challenging tasks using RealAPIExecutor:

**Previously tested (5 tasks):**
- Task 11: Add type hints to executor.py (complexity 4.5 → Haiku)
- Task 6: Add memoization to token counting (complexity 5.0 → Sonnet)
- Task 9: Write integration test for routing (complexity 5.5 → Sonnet)
- Task 7: Add error handling to MCP server (complexity 6.5 → Sonnet)
- Task 13: Implement input sanitization (complexity 7.0 → Opus)

**Never tested (10 tasks):**
- Task 8: Refactor scoring.py for extensibility (complexity 6.0 → Sonnet)
- Task 10: Add caching to dependency resolution (complexity 5.5 → Sonnet)
- Task 12: Implement retry logic with exponential backoff (complexity 6.5 → Sonnet)
- Task 14: Add comprehensive logging (complexity 5.0 → Sonnet)
- Task 15: Create benchmark suite (complexity 6.0 → Sonnet)
- Task 16: Implement rate limiting (complexity 6.5 → Sonnet)
- Task 17: Add metrics collection (complexity 5.5 → Sonnet)
- Task 18: Optimize graph traversal (complexity 7.5 → Opus)
- Task 19: Add streaming support (complexity 7.0 → Opus)
- Task 20: Implement transaction support (complexity 8.0 → Opus)

For each task:
1. Use orchestrator complexity scorer to determine model tier
2. Execute BASELINE with Sonnet (or Opus for 8+ complexity)
3. Execute OPTIMIZED with orchestrator-selected model
4. Measure tokens, cost, time
5. Score quality: 100% (perfect), 75% (mostly working), 50% (partial), 25% (broken), 0% (failure)
6. Log evidence with quality justification

Use actual codebase files as context. Quality scoring must be based on real code analysis, not assumptions.</action>
  <verify>pytest audit/tests/test_challenging_validation.py -v shows 15 tests executed with real API calls</verify>
  <done>All 15 challenging tasks executed with real Claude API, quality scored based on actual output analysis, evidence logged</done>
</task>

<task type="auto">
  <name>Task 2: Validate quality claims across complexity tiers</name>
  <files>audit/reports/05-02-quality-validation.md</files>
  <action>Analyze quality results and compare to claimed success rates:

1. Load real execution results from audit/evidence/challenging_validation_*.jsonl
2. Calculate success rate by complexity tier:
   - Simple (0-5): Expected 100% → Actual ?
   - Moderate (5-7): Expected 80-90% → Actual ?
   - Complex (7-8): Expected 60-75% → Actual ?
   - Critical (8-10): Expected varies → Actual ?

3. Identify patterns:
   - Which tasks failed and why?
   - Does Haiku handle 3-5 complexity reliably?
   - Does Sonnet handle 5-8 complexity reliably?
   - Does Opus handle 8-10 complexity reliably?
   - Any quality degradation from using cheaper models?

4. Compare to claimed 67% overall challenging task success:
   - Claimed: 3/5 fully working (60%), 2/5 partial (40%)
   - Actual: ?/15 fully working, ?/15 partial
   - Discrepancy explanation

5. Security task verification:
   - Task 13 claimed 0% (security failure)
   - Re-execution with Opus: Still 0% or improved?
   - Validate claim: "Don't use AI for security without expert review"

Include verdict: QUALITY CLAIMS VALIDATED / QUALITY INFLATED / MODEL ROUTING FLAWED with evidence and task-specific examples.</action>
  <verify>cat audit/reports/05-02-quality-validation.md shows success rates by tier, comparison to claimed 67%, and quality degradation analysis</verify>
  <done>Quality validation report exists with tier-specific success rates, comparison to claimed metrics, pattern analysis, and verdict on quality claims</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] All 15 challenging tasks executed with real Claude API
- [ ] Quality scores based on actual code analysis (not theoretical)
- [ ] Success rates calculated by complexity tier
- [ ] Comparison to claimed 67% success rate documented
- [ ] Pattern analysis identifies which models/tiers work reliably
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- 15 challenging tasks re-executed with real APIs
- Quality scored objectively based on actual outputs
- Success rate comparison to claimed 67% documented
- Model tier reliability assessed
</success_criteria>

<output>
After completion, create `.planning/phases/05-validation-metrics-reexecution/05-02-SUMMARY.md`:

# Phase 5 Plan 02: Challenging Tasks Re-execution Summary

**[One-liner about quality validation results]**

## Accomplishments

- 15 challenging tasks executed with real Claude API
- Quality scoring based on actual output analysis
- Success rate validation by complexity tier
- Model reliability patterns identified

## Files Created/Modified

- `audit/tests/test_challenging_validation.py` - Challenging task validation suite
- `audit/reports/05-02-quality-validation.md` - Quality claims validation report
- `audit/evidence/challenging_validation_*.jsonl` - Evidence from challenging tasks

## Decisions Made

[Findings about quality claims, model tier reliability, any surprising results]

## Issues Encountered

[Tasks that failed, quality issues found, if any]

## Next Step

Ready for 05-03-PLAN.md (Metrics Comparison & Analysis)
</output>
