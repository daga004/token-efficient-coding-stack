---
phase: 05-validation-metrics-reexecution
plan: 03
type: execute
---

<objective>
Calculate aggregate metrics from real API execution and compare to claimed 79.5% cost savings and quality metrics.

Purpose: Determine if published metrics (79.5% savings, 100% simple quality, 67% challenging quality) are validated by actual API execution.
Output: Comprehensive comparison report with aggregate statistics, discrepancy analysis, and validation verdict.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Prior work from Phase 5
@.planning/phases/05-validation-metrics-reexecution/05-01-SUMMARY.md
@.planning/phases/05-validation-metrics-reexecution/05-02-SUMMARY.md

# Evidence from real execution
@audit/evidence/simple_validation_*.jsonl
@audit/evidence/challenging_validation_*.jsonl

# Claimed results to compare against
@VALIDATION-SUMMARY.md
@.planning/phases/03-integration-validation/BASELINE-RESULTS.md
@.planning/phases/03-integration-validation/OPTIMIZED-RESULTS.md
@.planning/phases/03-integration-validation/VALIDATION-REPORT.md

**Key claims to verify:**
- 79.5% cost savings (Claude-only routing) on all tasks
- 100% quality on simple tasks (10/10)
- 67% quality on challenging tasks (3/5 fully working, 2/5 partial)
- 23% token savings (acknowledged as below target)
- Cost savings consistent 71-99% range across tasks

**From STATE.md:**
- Phase 3 validation claimed conditional pass (cost target exceeded, token target missed)
- Gap identified: Were costs real or theoretical?
- Need to determine: Do real API results support or refute these claims?
</context>

<tasks>

<task type="auto">
  <name>Task 1: Calculate aggregate metrics from real execution</name>
  <files>audit/aggregate_metrics.py</files>
  <action>Create AggregateMetricsCalculator that processes all evidence files and computes statistics:

1. Load all evidence from simple + challenging validation:
   - audit/evidence/simple_validation_*.jsonl (10 tasks)
   - audit/evidence/challenging_validation_*.jsonl (15 tasks)
   - Parse JSONL entries to extract tokens, costs, quality scores

2. Calculate baseline metrics (traditional approach):
   - Total input tokens (baseline)
   - Total output tokens (baseline)
   - Total cost (baseline, using Sonnet for simple, Sonnet/Opus for challenging)
   - Average time per task (baseline)

3. Calculate optimized metrics (AuZoom + Orchestrator):
   - Total input tokens (optimized, with progressive disclosure)
   - Total output tokens (optimized)
   - Total cost (optimized, with model routing)
   - Average time per task (optimized)

4. Calculate savings:
   - Token savings % = (baseline_tokens - optimized_tokens) / baseline_tokens * 100
   - Cost savings % = (baseline_cost - optimized_cost) / baseline_cost * 100
   - Time savings % = (baseline_time - optimized_time) / baseline_time * 100

5. Calculate success rates:
   - Simple tasks: Count with quality >= 75% / 10 * 100
   - Challenging tasks: Count with quality >= 75% / 15 * 100
   - Overall: Count with quality >= 75% / 25 * 100

6. Calculate by-tier performance:
   - Flash (tier 0): tasks, avg cost savings, avg quality
   - Haiku (tier 1): tasks, avg cost savings, avg quality
   - Sonnet (tier 2): tasks, avg cost savings, avg quality
   - Opus (tier 3): tasks, avg cost savings, avg quality

Output JSON with all calculated metrics for easy consumption.</action>
  <verify>python audit/aggregate_metrics.py outputs JSON with calculated aggregate metrics from real execution</verify>
  <done>AggregateMetricsCalculator exists, processes all evidence files, outputs aggregate metrics including savings percentages and success rates</done>
</task>

<task type="auto">
  <name>Task 2: Compare actual metrics to claimed metrics</name>
  <files>audit/reports/05-03-metrics-comparison.md</files>
  <action>Generate comprehensive comparison report between claimed and actual metrics:

## 1. Cost Savings Comparison

| Metric | Claimed | Actual | Discrepancy | Verdict |
|--------|---------|--------|-------------|---------|
| Overall cost savings | 79.5% | ? | ? | VALIDATED/REFUTED |
| Simple tasks cost savings | 81% | ? | ? | VALIDATED/REFUTED |
| Challenging tasks cost savings | ~52.5% | ? | ? | VALIDATED/REFUTED |
| Cost savings range | 71-99% | ? | ? | VALIDATED/REFUTED |

## 2. Token Savings Comparison

| Metric | Claimed | Actual | Discrepancy | Verdict |
|--------|---------|--------|-------------|---------|
| Overall token savings | 23% | ? | ? | VALIDATED/REFUTED |
| Simple tasks token savings | 23% | ? | ? | VALIDATED/REFUTED |
| Challenging tasks token savings | 48% | ? | ? | VALIDATED/REFUTED |

## 3. Quality Comparison

| Metric | Claimed | Actual | Discrepancy | Verdict |
|--------|---------|--------|-------------|---------|
| Simple tasks success | 100% (10/10) | ? | ? | VALIDATED/REFUTED |
| Challenging tasks success | 67% (3/5 + 2 partial) | ? | ? | VALIDATED/REFUTED |
| Overall success | ~90% | ? | ? | VALIDATED/REFUTED |

## 4. By-Tier Performance

For each tier (Flash, Haiku, Sonnet, Opus):
- Claimed task count vs actual
- Claimed cost savings vs actual
- Claimed quality vs actual

## 5. Discrepancy Analysis

For each metric with >10% discrepancy:
- Root cause (theoretical vs real, measurement error, methodology flaw)
- Impact (claim inflated, claim deflated, claim accurate)
- Recommendation (accept claim, revise claim, investigate further)

## 6. Overall Verdict

**79.5% Cost Savings Claim:** VALIDATED / PARTIALLY VALIDATED / REFUTED
**100% Simple Quality Claim:** VALIDATED / PARTIALLY VALIDATED / REFUTED
**67% Challenging Quality Claim:** VALIDATED / PARTIALLY VALIDATED / REFUTED

**Recommendation:** [Accept claims / Revise claims / Re-validate with different methodology]

Include evidence references for all findings (which evidence files, which tasks, specific numbers).</action>
  <verify>cat audit/reports/05-03-metrics-comparison.md shows claimed vs actual comparison tables with verdicts</verify>
  <done>Metrics comparison report exists with comprehensive claimed vs actual analysis, discrepancy explanations, and overall validation verdict</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] Aggregate metrics calculated from all 25 tasks' real API execution
- [ ] All major claims compared (79.5% savings, 100%/67% quality)
- [ ] Discrepancies quantified and explained
- [ ] Verdicts provided for each claim (validated/refuted)
- [ ] Evidence references included for all findings
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- Aggregate metrics calculated from real API data
- Comprehensive comparison to claimed metrics
- Verdicts on all major claims with evidence
- Discrepancy analysis with root cause explanations
</success_criteria>

<output>
After completion, create `.planning/phases/05-validation-metrics-reexecution/05-03-SUMMARY.md`:

# Phase 5 Plan 03: Metrics Comparison Summary

**[One-liner about validation verdict - validated/refuted]**

## Accomplishments

- Aggregate metrics calculated from 25 real API executions
- Comprehensive claimed vs actual comparison
- Verdicts on cost savings, token savings, quality claims
- Discrepancy analysis with root causes

## Files Created/Modified

- `audit/aggregate_metrics.py` - Aggregate metrics calculator
- `audit/reports/05-03-metrics-comparison.md` - Claimed vs actual comparison report

## Key Findings

**Cost Savings (79.5% claimed):** [Actual result, verdict]
**Simple Quality (100% claimed):** [Actual result, verdict]
**Challenging Quality (67% claimed):** [Actual result, verdict]

## Decisions Made

[Which claims validated, which refuted, implications for V1 certification]

## Issues Encountered

[Major discrepancies found, if any]

## Next Step

Ready for 05-04-PLAN.md (Methodology Assessment)
</output>
