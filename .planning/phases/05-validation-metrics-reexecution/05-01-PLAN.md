---
phase: 05-validation-metrics-reexecution
plan: 01
type: execute
---

<objective>
Re-execute 10 simple validation tasks with real Claude API calls to verify claimed token/cost savings.

Purpose: Replace theoretical/simulated measurements with actual API execution data to validate or refute 79.5% cost savings claim.
Output: Real API execution results for tasks 1.1-5.2, evidence files with actual tokens/costs, comparison report identifying discrepancies.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Prior phase context
@.planning/phases/01-audit-foundation-traceability/01-02-SUMMARY.md
@.planning/phases/03-integration-validation/03-02-SUMMARY.md

# Test infrastructure
@audit/harness.py
@audit/models.py
@audit/logger.py

# Validation test definitions
@.planning/phases/03-integration-validation/TEST-SUITE.md
@.planning/phases/03-integration-validation/BASELINE-RESULTS.md
@.planning/phases/03-integration-validation/OPTIMIZED-RESULTS.md

# Orchestrator for API execution
@orchestrator/src/orchestrator/executor.py
@orchestrator/src/orchestrator/registry.py
@orchestrator/src/orchestrator/scoring.py

**Key context from STATE.md:**
- Original validation: 10 simple tasks, 83% cost savings claimed, 100% quality
- Token savings: 23% (below 50% target due to small file bias)
- Quality: 100% success rate claimed
- **Gap**: Were these real API calls or theoretical costs?

**From Phase 3 validation:**
- Tasks 1.1-5.2 defined in TEST-SUITE.md
- BASELINE-RESULTS.md shows Sonnet-only approach
- OPTIMIZED-RESULTS.md shows AuZoom+Orchestrator with model routing
- Need to verify: Were APIs actually called or were costs calculated theoretically?
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extend audit harness for real API execution</name>
  <files>audit/real_api_executor.py</files>
  <action>Create RealAPIExecutor class that extends audit/harness.py AuditTest base. Must make actual Claude API calls (not simulations). Include:
  - anthropic Python SDK integration (use environment ANTHROPIC_API_KEY)
  - execute_task(task_description, model, context_files) method that sends real API request
  - Token counting from API response (usage.input_tokens, usage.output_tokens)
  - Cost calculation from real pricing (Flash $0.01/1M, Haiku $0.80/1M, Sonnet $3.00/1M, Opus $15.00/1M for input)
  - Quality scoring: compare output to expected behavior (100%/75%/50%/0%)
  - Evidence logging via self.evidence.log()

  Use synchronous API calls (not async) for simplicity. Handle rate limits with exponential backoff. Do NOT use theoretical/estimated costs - only actual API responses count.</action>
  <verify>python -c "from audit.real_api_executor import RealAPIExecutor; r = RealAPIExecutor('test', 'simple'); print('Imports work')"</verify>
  <done>RealAPIExecutor class exists, imports successfully, has execute_task method accepting task description and returning tokens/cost/quality from real API call</done>
</task>

<task type="auto">
  <name>Task 2: Execute 10 simple tasks with real Claude API</name>
  <files>audit/tests/test_simple_validation.py</files>
  <action>Create test suite that executes all 10 simple tasks (1.1-5.2) from TEST-SUITE.md using RealAPIExecutor:

For each task:
1. Read task definition from TEST-SUITE.md (baseline approach, optimized approach)
2. Execute BASELINE approach with real Claude API (Sonnet model)
3. Execute OPTIMIZED approach with real Claude API (using orchestrator to select model: Haiku/Flash based on complexity)
4. Measure: input_tokens, output_tokens, cost, time, quality_score
5. Log evidence to audit/evidence/simple_validation_{timestamp}.jsonl

Tasks to execute:
- 1.1: Explore AuZoom codebase (complexity 2.5 → Haiku)
- 1.2: Find score_task function (complexity 2.0 → Haiku)
- 2.1: Fix typo in docstring (complexity 1.5 → Flash)
- 2.2: Update MAX_TOKENS constant (complexity 0.5 → Flash)
- 3.1: Add validation rule (complexity 5.0 → Haiku)
- 3.2: Add cost tracking (complexity 5.5 → Sonnet)
- 4.1: Extract helper function (complexity 4.5 → Haiku)
- 4.2: Rename module (complexity 3.5 → Haiku)
- 5.1: Diagnose test failure (complexity 4.5 → Haiku)
- 5.2: Fix circular import (complexity 5.0 → Haiku)

Use actual project files (auzoom/, orchestrator/) as context. Compare real API token counts to claimed baseline/optimized numbers.</action>
  <verify>pytest audit/tests/test_simple_validation.py -v shows 10 tests executed with real API calls</verify>
  <done>All 10 simple tasks executed with real Claude API, evidence logged with actual tokens/costs/quality, test suite passes</done>
</task>

<task type="auto">
  <name>Task 3: Compare real results to claimed metrics</name>
  <files>audit/reports/05-01-simple-tasks-comparison.md</files>
  <action>Generate comparison report analyzing real vs claimed results:

1. Load claimed results from BASELINE-RESULTS.md and OPTIMIZED-RESULTS.md
2. Load real results from audit/evidence/simple_validation_*.jsonl
3. For each task, create comparison table:
   - Task ID
   - Claimed baseline tokens/cost
   - Actual baseline tokens/cost
   - Claimed optimized tokens/cost
   - Actual optimized tokens/cost
   - Discrepancy % (actual vs claimed)
   - Notes on differences

4. Calculate aggregates:
   - Total claimed cost savings: 83%
   - Total actual cost savings: ?
   - Difference and explanation

5. Identify methodology issues:
   - Were claimed costs theoretical or real?
   - Were token counts estimated or measured?
   - Were baseline costs fair (Sonnet universally appropriate)?

Include verdict: METRICS VALIDATED / METRICS INFLATED / METHODOLOGY FLAWED with evidence.</action>
  <verify>cat audit/reports/05-01-simple-tasks-comparison.md shows task-by-task comparison with actual vs claimed discrepancies</verify>
  <done>Comparison report exists with real vs claimed data, aggregate metrics, discrepancy analysis, and methodology assessment</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] RealAPIExecutor makes actual Claude API calls (verified by checking API logs or response objects)
- [ ] All 10 simple tasks executed with real APIs (not simulations)
- [ ] Evidence files contain actual token counts and costs from API responses
- [ ] Comparison report documents real vs claimed discrepancies
- [ ] No theoretical/estimated costs used (only actual API measurements)
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- 10 simple tasks re-executed with real Claude API
- Evidence logged to audit/evidence/ with actual API response data
- Comparison report identifies discrepancies between claimed and actual metrics
- Methodology issues documented (if any)
</success_criteria>

<output>
After completion, create `.planning/phases/05-validation-metrics-reexecution/05-01-SUMMARY.md`:

# Phase 5 Plan 01: Simple Tasks Re-execution Summary

**[One-liner about actual vs claimed results]**

## Accomplishments

- RealAPIExecutor for actual Claude API calls
- 10 simple tasks re-executed with real APIs
- Evidence collected from actual API responses
- Comparison report with discrepancy analysis

## Files Created/Modified

- `audit/real_api_executor.py` - Real API execution harness
- `audit/tests/test_simple_validation.py` - Simple task validation suite
- `audit/reports/05-01-simple-tasks-comparison.md` - Real vs claimed comparison
- `audit/evidence/simple_validation_*.jsonl` - Evidence from real API execution

## Decisions Made

[Key findings about methodology, any discrepancies found]

## Issues Encountered

[Problems during execution, if any]

## Next Step

Ready for 05-02-PLAN.md (Re-execute Challenging Tasks)
</output>
