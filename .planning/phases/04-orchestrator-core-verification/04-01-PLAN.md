---
phase: 04-orchestrator-core-verification
plan: 01
type: execute
---

<objective>
Test complexity scorer accuracy by creating comprehensive edge case test suite and validating scores against real validation tasks.

Purpose: Verify Assumption 2 - that the complexity scorer accurately assesses task difficulty and produces appropriate 0-10 scores matching actual task complexity.
Output: Test suite with edge cases, accuracy report comparing scores to actual difficulty, misclassification analysis.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Prior phase context
@.planning/phases/02-orchestrator-implementation/02-01-SUMMARY.md
@.planning/phases/03-integration-validation/03-02-SUMMARY.md

# Source code
@orchestrator/src/orchestrator/scoring.py
@orchestrator/src/orchestrator/models.py

# Validation data
@.planning/phases/03-integration-validation/TEST-SUITE.md
@.planning/phases/03-integration-validation/OPTIMIZED-RESULTS.md

**Tech stack available:** ComplexityScorer (7 weighted factors), TaskComplexity model, pytest test framework

**Established patterns:** Rule-based scoring (0-10 scale), factor breakdown, confidence calculation, tier mapping (0-3→Flash, 3-5→Haiku, 5-8→Sonnet, 8-10→Opus)

**Constraining decisions:**
- Phase 2: Deterministic scoring ensures explainability and reproducibility
- Phase 2: Factor-based confidence reflects number of active factors
- Phase 3: Validation showed 100% quality maintained across tiers

**Issues being addressed:** Verify complexity scorer accuracy against real-world task difficulty (audit requirement)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create edge case test suite for complexity scorer</name>
  <files>audit/tests/test_scorer_edge_cases.py</files>
  <action>Create comprehensive test suite covering edge cases and boundary conditions:
  - Boundary testing: Tasks scoring exactly 0, 3, 5, 8, 10 (tier boundaries)
  - Single-factor dominance: Tasks triggered by only one factor (length, keywords, file count, etc.)
  - Multi-factor combination: Tasks with multiple overlapping factors
  - Confidence extremes: Tasks with 0.5 confidence (1 factor) vs 1.0 confidence (all 7 factors)
  - Keyword detection: Case sensitivity, partial matches, context (e.g., "authentication" in different contexts)
  - File count thresholds: 0 files, 1 file, 5 files, 10+ files
  - Empty/minimal tasks: Empty description, single word, no context
  - Maximum complexity: All factors maxed out (should cap at 10.0)

  Use pytest framework. Import ComplexityScorer and Task from orchestrator. Create 15-20 test cases covering all edge conditions. Assert both score values and tier mappings are correct.
  </action>
  <verify>pytest audit/tests/test_scorer_edge_cases.py -v passes all tests</verify>
  <done>15+ edge case tests created, all passing, boundary conditions and factor combinations covered</done>
</task>

<task type="auto">
  <name>Task 2: Test scorer accuracy against validation task suite</name>
  <files>audit/tests/test_scorer_accuracy.py</files>
  <action>Create test that validates complexity scores against the 10 validation tasks from TEST-SUITE.md:

  For each task (1.1, 1.2, 2.1, 2.2, 3.1, 3.2, 4.1, 4.2, 5.1, 5.2):
  1. Extract task description and context from TEST-SUITE.md
  2. Create Task object with description and relevant context (files_count, requires_tests, etc.)
  3. Score task using ComplexityScorer
  4. Compare score to actual task difficulty observed in validation:
     - Tasks 1.1-2.2 (exploration, simple edits): Should score 0-5 (Flash/Haiku)
     - Tasks 3.1-3.2 (features): Should score 4-7 (Haiku/Sonnet)
     - Tasks 4.1-4.2 (refactoring): Should score 5-8 (Sonnet)
     - Tasks 5.1-5.2 (debugging): Should score 4-7 (Haiku/Sonnet)
  5. Flag mismatches where scored tier doesn't match actual model used in OPTIMIZED-RESULTS.md

  Create accuracy metrics: % correct tier assignments, average score deviation, confusion matrix showing predicted vs actual tier.

  Store results in audit/evidence/scorer_accuracy_YYYYMMDD_HHMMSS.jsonl format with fields: task_id, description, predicted_score, predicted_tier, actual_tier, match (bool), factors.
  </action>
  <verify>pytest audit/tests/test_scorer_accuracy.py -v passes, accuracy metrics generated, evidence logged</verify>
  <done>All 10 validation tasks scored, tier accuracy calculated, mismatches identified, evidence saved to JSONL</done>
</task>

<task type="auto">
  <name>Task 3: Document scorer accuracy and create analysis report</name>
  <files>audit/reports/04-01-SCORER-ACCURACY.md</files>
  <action>Create comprehensive scorer accuracy report:

## Complexity Scorer Accuracy Report

### Executive Summary
- Overall accuracy: [X]% tier matches
- Average score deviation: [X] points
- Confidence correlation: Does higher confidence correlate with accuracy?

### Edge Case Testing Results
- Total edge cases: [X]
- Passed: [X]
- Failed: [X]
- Key findings: [list any unexpected behaviors]

### Validation Task Accuracy
Present table:
| Task | Description | Predicted | Actual | Match | Score | Factors |
|------|-------------|-----------|--------|-------|-------|---------|
| 1.1  | Explore pkg | Tier X    | Tier Y | ✓/✗   | N.NN  | {...}   |
[... all 10 tasks]

### Misclassification Analysis
For each mismatch:
- **Task ID**: What went wrong?
- **Root cause**: Which factors over/under-weighted?
- **Impact**: Did quality suffer? Did cost increase unnecessarily?
- **Recommendation**: Adjust factor weights? Add new factor?

### Accuracy by Task Category
- Exploration: [X]% accuracy
- Simple edits: [X]% accuracy
- Features: [X]% accuracy
- Refactoring: [X]% accuracy
- Debugging: [X]% accuracy

### Overall Assessment
- Is scorer accurate enough for production routing? (threshold: ≥80% tier match)
- Are misclassifications causing quality degradation or cost waste?
- Should factor weights be adjusted?

### Recommendations
[List specific improvements if accuracy <80%, or "No changes needed" if ≥80%]
  </action>
  <verify>Report exists, contains all sections with actual data from tests, includes specific recommendations</verify>
  <done>Comprehensive accuracy report created with metrics, misclassification analysis, and actionable recommendations</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] pytest audit/tests/test_scorer_edge_cases.py passes all tests
- [ ] pytest audit/tests/test_scorer_accuracy.py passes, evidence logged
- [ ] audit/reports/04-01-SCORER-ACCURACY.md exists with complete analysis
- [ ] Accuracy metrics calculated: tier match %, score deviation, confusion matrix
- [ ] Misclassifications identified and analyzed
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- Edge case test suite covers boundaries and factor combinations
- Validation task accuracy measured against actual results
- Scorer accuracy report documents findings with evidence
- Recommendations provided if accuracy <80%
</success_criteria>

<output>
After completion, create `.planning/phases/04-orchestrator-core-verification/04-01-SUMMARY.md`:

---
phase: 04-orchestrator-core-verification
plan: 01
subsystem: orchestrator-audit
tags: [complexity-scoring, accuracy-testing, validation, edge-cases]

# Dependency graph
requires:
  - phase: 02-orchestrator-implementation
    provides: ComplexityScorer implementation
  - phase: 03-integration-validation
    provides: 10 validation tasks with actual routing
provides:
  - Scorer edge case test suite
  - Accuracy metrics (tier match %, confusion matrix)
  - Misclassification analysis
  - Scorer improvement recommendations
affects: [04-02-model-routing-appropriateness, 05-validation-metrics-reexecution, 11-gap-analysis-reporting]

# Tech tracking
tech-stack:
  added: []
  patterns: [edge-case-testing, accuracy-measurement, confusion-matrix]

key-files:
  created: [audit/tests/test_scorer_edge_cases.py, audit/tests/test_scorer_accuracy.py, audit/reports/04-01-SCORER-ACCURACY.md]
  modified: []

key-decisions:
  - "Accuracy threshold: ≥80% tier match rate"
  - "Evidence format: JSONL for scorer accuracy validation"
  - "Misclassification analysis includes root cause and impact assessment"

patterns-established:
  - "Edge case testing covers boundaries, single-factor, multi-factor, extremes"
  - "Accuracy testing compares predicted tier to actual model used"
  - "Confusion matrix format for tier prediction analysis"

issues-created: []

# Metrics
duration: [X]min
completed: YYYY-MM-DD
---

# Phase 4 Plan 01: Complexity Scorer Accuracy Testing Summary

**[One-liner describing scorer accuracy results - e.g., "Scorer achieves 90% tier accuracy with minor misclassifications on edge cases"]**

## Accomplishments

- [Key achievement 1]
- [Key achievement 2]
- [Key achievement 3]

## Files Created/Modified

- `audit/tests/test_scorer_edge_cases.py` - [X] edge case tests
- `audit/tests/test_scorer_accuracy.py` - Validation task accuracy testing
- `audit/reports/04-01-SCORER-ACCURACY.md` - Comprehensive accuracy report
- `audit/evidence/scorer_accuracy_*.jsonl` - Evidence logging

## Decisions Made

[Decisions made during execution, or "None"]

## Issues Encountered

[Problems and resolutions, or "None"]

## Next Step

Ready for 04-02-PLAN.md (Model Routing Appropriateness Assessment)
</output>
