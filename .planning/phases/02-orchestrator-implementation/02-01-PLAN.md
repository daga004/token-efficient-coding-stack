---
phase: 02-orchestrator-implementation
plan: 01
type: execute
---

<objective>
Implement complexity scoring engine and model registry for intelligent task routing.

Purpose: Enable rule-based complexity assessment (0-10 scale) that maps tasks to appropriate models (Gemini Flash/Pro, Haiku, Sonnet, Opus) based on objective criteria, eliminating need for LLM-based routing decisions.

Output: Working complexity scorer with test coverage, model registry with cost/capability profiles, foundation for orchestrator routing logic.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-phase.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-auzoom-implementation/01-01-SUMMARY.md

**Existing research:**
@/Users/dhirajd/Documents/claude/unified-orchestrator/orchestrator-architecture-analysis.md
@/Users/dhirajd/Documents/claude/unified-orchestrator/hierarchical-orchestrator/SKILL.md

**Use AuZoom to navigate existing code:**
- Use `auzoom_read` with skeleton/summary levels to understand AuZoom's structure
- Reference AuZoom's models.py for Pydantic patterns
- Reference AuZoom's MCP implementation patterns (will help in Plan 02-03)

**Tech stack available:**
- Python 3.10+, Pydantic for data models
- tree-sitter patterns from AuZoom Phase 1
- MCP SDK patterns (from AuZoom implementation)

**Model Tier Strategy (updated with Gemini):**
- **Tier 0 (0-3):** Gemini Flash - $0.01/1M tokens - Simple tasks, docs, tests
- **Tier 1 (3-5):** Haiku - $0.80/1M tokens - Moderate coding tasks
- **Tier 2 (5-8):** Sonnet - $3.00/1M tokens - Complex tasks, validation
- **Tier 3 (8-10):** Opus - $15/1M tokens - Critical/architectural decisions

**Constraining decisions:**
- No local models (simplified for V1)
- Rule-based scoring only (no ML/LLM for routing)
- Gemini CLI integration (not API initially)
- Follow AuZoom's code standards (≤250 lines/module, ≤50 lines/function)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement complexity scoring function</name>
  <files>orchestrator/src/orchestrator/scoring.py, orchestrator/src/orchestrator/models.py</files>
  <action>
Create orchestrator package structure:
```
orchestrator/
├── src/orchestrator/
│   ├── __init__.py
│   ├── models.py      # Pydantic models
│   ├── scoring.py     # Complexity scoring
│   └── registry.py    # Model registry (next task)
├── tests/
│   └── test_scoring.py
└── pyproject.toml
```

In models.py, define:
```python
from pydantic import BaseModel
from enum import Enum

class TaskComplexity(BaseModel):
    score: float  # 0-10
    factors: dict[str, float]  # breakdown
    recommended_tier: int  # 0-3
    confidence: float  # 0-1

class Task(BaseModel):
    description: str
    context: dict  # files_count, requires_tests, etc.
```

In scoring.py, implement rule-based complexity scorer:
- Task length factor (0-3 points based on word count)
- Keyword detection: "refactor", "architect", "migrate" (+2), "auth", "payment", "security" (+2)
- File count: >3 files (+1.5), >5 files (+2)
- Requires tests (+1.5)
- Dependencies on external APIs (+1.5)
- Multi-subsystem work (+2)
- Return TaskComplexity with score, breakdown, tier

Use AuZoom patterns: Reference auzoom/src/auzoom/models.py (via auzoom_read at skeleton level) for Pydantic enum patterns, keep functions ≤50 lines.

Test with 10 sample tasks covering complexity range 0-10.
  </action>
  <verify>pytest tests/test_scoring.py passes with 10+ test cases covering all complexity tiers</verify>
  <done>Complexity scorer returns accurate 0-10 scores with factor breakdown, tests validate edge cases, code follows AuZoom standards (≤250 lines/module)</done>
</task>

<task type="auto">
  <name>Task 2: Create model registry with tier profiles</name>
  <files>orchestrator/src/orchestrator/registry.py</files>
  <action>
Implement ModelRegistry class:

```python
class ModelTier(Enum):
    FLASH = "gemini-flash"    # Tier 0
    PRO = "gemini-pro"        # Tier 0-1 (if Flash fails)
    HAIKU = "haiku"           # Tier 1
    SONNET = "sonnet"         # Tier 2
    OPUS = "opus"             # Tier 3

class ModelProfile(BaseModel):
    name: str
    tier: int
    cost_per_1m_input: float
    cost_per_1m_output: float
    max_tokens: int
    best_for: list[str]  # task types

class ModelRegistry:
    def __init__(self):
        self.models = {
            ModelTier.FLASH: ModelProfile(
                name="Gemini Flash",
                tier=0,
                cost_per_1m_input=0.01,
                cost_per_1m_output=0.04,
                max_tokens=8192,
                best_for=["simple_code", "tests", "docs", "comments"]
            ),
            ModelTier.HAIKU: ModelProfile(
                name="Claude Haiku",
                tier=1,
                cost_per_1m_input=0.80,
                cost_per_1m_output=4.00,
                max_tokens=200000,
                best_for=["moderate_code", "refactoring", "api_endpoints"]
            ),
            ModelTier.SONNET: ModelProfile(
                name="Claude Sonnet",
                tier=2,
                cost_per_1m_input=3.00,
                cost_per_1m_output=15.00,
                max_tokens=200000,
                best_for=["complex_logic", "validation", "architecture_review"]
            ),
            ModelTier.OPUS: ModelProfile(
                name="Claude Opus",
                tier=3,
                cost_per_1m_input=15.00,
                cost_per_1m_output=75.00,
                max_tokens=200000,
                best_for=["critical_decisions", "system_design", "security_audit"]
            )
        }

    def get_model_for_score(self, complexity_score: float) -> ModelTier:
        """Map complexity score to model tier."""
        if complexity_score < 3: return ModelTier.FLASH
        if complexity_score < 5: return ModelTier.HAIKU
        if complexity_score < 8: return ModelTier.SONNET
        return ModelTier.OPUS

    def estimate_cost(self, model: ModelTier, input_tokens: int, output_tokens: int) -> float:
        """Calculate estimated cost for task."""
        profile = self.models[model]
        return (input_tokens * profile.cost_per_1m_input / 1_000_000) + \
               (output_tokens * profile.cost_per_1m_output / 1_000_000)
```

Keep module ≤250 lines. Add tests for registry lookups and cost estimation.
  </action>
  <verify>pytest tests/test_registry.py passes, all 4 model tiers defined with accurate costs, score-to-tier mapping works correctly</verify>
  <done>Model registry complete with profiles, cost estimation working, tier selection logic tested, follows AuZoom code standards</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `pytest orchestrator/tests/` passes all tests
- [ ] Complexity scorer handles edge cases (empty task, extreme complexity)
- [ ] Model registry returns correct tier for scores at boundaries (2.9→Flash, 3.1→Haiku, etc.)
- [ ] All code follows AuZoom standards (≤250 lines/module, ≤50 lines/function)
- [ ] No external API calls yet (pure logic)
</verification>

<success_criteria>
- All tasks completed
- All tests pass (10+ scoring tests, 5+ registry tests)
- Complexity scoring deterministic and explainable
- Model tier mapping aligns with cost optimization goals
- Code compliant with project standards
- Foundation ready for dispatch layer (Plan 02-02)
</success_criteria>

<output>
After completion, create `.planning/phases/02-orchestrator-implementation/02-01-SUMMARY.md`:

# Phase 2 Plan 01: Complexity Scoring & Model Registry Summary

**[One-liner describing what shipped]**

## Accomplishments

- Complexity scoring engine with rule-based 0-10 scale
- Model registry with 4-tier system (Gemini Flash, Haiku, Sonnet, Opus)
- Cost estimation calculator
- Comprehensive test coverage

## Files Created/Modified

- `orchestrator/src/orchestrator/models.py` - Pydantic models for Task, TaskComplexity, ModelProfile
- `orchestrator/src/orchestrator/scoring.py` - Rule-based complexity scorer
- `orchestrator/src/orchestrator/registry.py` - Model registry with tier mapping
- `orchestrator/tests/test_*.py` - Test coverage
- `orchestrator/pyproject.toml` - Package configuration

## Commits

- [List commit SHAs and messages]

## Decisions Made

[Key decisions and rationale]

## Issues Encountered

[Problems and resolutions, or "None"]

## Next Step

Ready for 02-02-PLAN.md (Model Dispatch Layer)
</output>
