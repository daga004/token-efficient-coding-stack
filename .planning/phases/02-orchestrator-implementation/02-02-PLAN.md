---
phase: 02-orchestrator-implementation
plan: 02
type: execute
---

<objective>
Implement model dispatch layer with Gemini CLI and Anthropic API clients for executing tasks on routed models.

Purpose: Enable orchestrator to execute tasks on Gemini Flash/Pro (via CLI) and Anthropic models (Haiku/Sonnet/Opus via API), with unified interface, error handling, and retry logic.

Output: Working dispatch layer with Gemini CLI wrapper, Anthropic API client, unified execution interface, retry/fallback logic, and test coverage.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-phase.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-orchestrator-implementation/02-01-SUMMARY.md

**Existing code from Plan 02-01:**
- Use `auzoom_read(path="orchestrator/src/orchestrator/models.py", level="summary")` to understand existing models
- Use `auzoom_read(path="orchestrator/src/orchestrator/registry.py", level="summary")` to see ModelRegistry interface

**Tech stack:**
- Gemini CLI: `gemini` command (install via: `pip install google-generativeai-cli` or similar)
- Anthropic API: `anthropic` Python SDK (install: `pip install anthropic`)
- Use AuZoom to reference error handling patterns from auzoom/src/auzoom/mcp/

**API Integration:**
- Gemini CLI: subprocess execution, capture stdout/stderr
- Anthropic: Official SDK with streaming support
- Both: unified response format, token counting, error handling

**Constraining decisions:**
- Gemini via CLI initially (not direct API) - simpler auth
- Anthropic via official SDK
- Unified ExecutionResult model for all clients
- Follow AuZoom standards (≤250 lines/module, ≤50 lines/function)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement Gemini CLI wrapper</name>
  <files>orchestrator/src/orchestrator/clients/gemini.py, orchestrator/src/orchestrator/clients/__init__.py</files>
  <action>
Create clients package:
```
orchestrator/src/orchestrator/clients/
├── __init__.py
├── gemini.py
├── anthropic.py  (next task)
└── base.py       (shared interfaces)
```

In base.py, define shared interface:
```python
from pydantic import BaseModel
from abc import ABC, abstractmethod

class ExecutionResult(BaseModel):
    model: str
    response: str
    tokens_input: int
    tokens_output: int
    latency_ms: int
    success: bool
    error: str | None = None

class ModelClient(ABC):
    @abstractmethod
    async def execute(self, prompt: str, max_tokens: int = 4096) -> ExecutionResult:
        pass
```

In gemini.py, implement GeminiClient:
```python
import subprocess
import json
import time

class GeminiClient(ModelClient):
    def __init__(self, model: str = "gemini-flash"):
        self.model = model

    async def execute(self, prompt: str, max_tokens: int = 8192) -> ExecutionResult:
        start = time.time()

        # Use gemini CLI: gemini generate --model=flash --prompt="..."
        try:
            result = subprocess.run(
                ["gemini", "generate", f"--model={self.model}",
                 f"--max-tokens={max_tokens}", "--prompt", prompt],
                capture_output=True,
                text=True,
                timeout=30
            )

            if result.returncode != 0:
                return ExecutionResult(
                    model=self.model,
                    response="",
                    tokens_input=0,
                    tokens_output=0,
                    latency_ms=int((time.time() - start) * 1000),
                    success=False,
                    error=result.stderr
                )

            # Parse response (format depends on gemini CLI output)
            response = result.stdout.strip()

            # Estimate tokens (rough: 4 chars = 1 token)
            tokens_input = len(prompt) // 4
            tokens_output = len(response) // 4

            return ExecutionResult(
                model=self.model,
                response=response,
                tokens_input=tokens_input,
                tokens_output=tokens_output,
                latency_ms=int((time.time() - start) * 1000),
                success=True
            )

        except subprocess.TimeoutExpired:
            return ExecutionResult(
                model=self.model,
                response="",
                tokens_input=0,
                tokens_output=0,
                latency_ms=30000,
                success=False,
                error="Timeout after 30s"
            )
        except FileNotFoundError:
            return ExecutionResult(
                model=self.model,
                response="",
                tokens_input=0,
                tokens_output=0,
                latency_ms=0,
                success=False,
                error="Gemini CLI not installed. Run: pip install google-generativeai"
            )
```

Keep module ≤250 lines. Use AuZoom patterns: reference auzoom/src/auzoom/mcp/server.py (via auzoom_read) for error handling patterns.

Test with mock subprocess calls (don't require actual Gemini CLI for tests).
  </action>
  <verify>pytest tests/test_gemini.py passes with mocked subprocess, ExecutionResult returned correctly for success/failure/timeout cases</verify>
  <done>Gemini CLI wrapper complete with timeout handling, error capture, token estimation, unified ExecutionResult format, tests passing</done>
</task>

<task type="auto">
  <name>Task 2: Implement Anthropic API client</name>
  <files>orchestrator/src/orchestrator/clients/anthropic.py</files>
  <action>
Install anthropic SDK in pyproject.toml dependencies: `anthropic>=0.18.0`

Implement AnthropicClient:
```python
from anthropic import Anthropic, APIError
import os
import time

class AnthropicClient(ModelClient):
    def __init__(self, model: str = "claude-3-5-haiku-20241022"):
        self.client = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
        self.model = model

        # Model name mapping
        self.model_map = {
            "haiku": "claude-3-5-haiku-20241022",
            "sonnet": "claude-3-5-sonnet-20241022",
            "opus": "claude-opus-4-20250514"
        }

    async def execute(self, prompt: str, max_tokens: int = 4096) -> ExecutionResult:
        start = time.time()

        model_id = self.model_map.get(self.model, self.model)

        try:
            response = self.client.messages.create(
                model=model_id,
                max_tokens=max_tokens,
                messages=[{"role": "user", "content": prompt}]
            )

            return ExecutionResult(
                model=self.model,
                response=response.content[0].text,
                tokens_input=response.usage.input_tokens,
                tokens_output=response.usage.output_tokens,
                latency_ms=int((time.time() - start) * 1000),
                success=True
            )

        except APIError as e:
            return ExecutionResult(
                model=self.model,
                response="",
                tokens_input=0,
                tokens_output=0,
                latency_ms=int((time.time() - start) * 1000),
                success=False,
                error=str(e)
            )
        except Exception as e:
            return ExecutionResult(
                model=self.model,
                response="",
                tokens_input=0,
                tokens_output=0,
                latency_ms=int((time.time() - start) * 1000),
                success=False,
                error=f"Unexpected error: {str(e)}"
            )
```

Add validation tool support (for orchestrator_validate):
```python
async def validate(self, task: str, output: str, max_tokens: int = 100) -> dict:
    """Sonnet-based validation (input-heavy mode)."""
    validation_prompt = f"""<context>{task}</context>
<output_to_validate>{output}</output_to_validate>

Validate the output against the task requirements.
Respond ONLY with JSON (max 100 tokens):
{{"pass": bool, "issues": ["max 3 items"], "confidence": 0-1, "escalate": bool}}"""

    result = await self.execute(validation_prompt, max_tokens=max_tokens)

    if not result.success:
        return {"pass": False, "issues": [result.error], "confidence": 0, "escalate": True}

    try:
        return json.loads(result.response)
    except json.JSONDecodeError:
        return {"pass": False, "issues": ["Invalid JSON response"], "confidence": 0, "escalate": True}
```

Keep module ≤250 lines. Test with mocked Anthropic client (don't require actual API key for tests).
  </action>
  <verify>pytest tests/test_anthropic.py passes with mocked API, all 3 models (haiku/sonnet/opus) work, validation method returns proper JSON</verify>
  <done>Anthropic client complete with proper SDK usage, model name mapping, validation support, error handling, tests passing</done>
</task>

<task type="auto">
  <name>Task 3: Create unified executor with retry and fallback</name>
  <files>orchestrator/src/orchestrator/executor.py</files>
  <action>
Implement Executor class that wraps clients with retry/fallback logic:

```python
from .clients.gemini import GeminiClient
from .clients.anthropic import AnthropicClient
from .registry import ModelRegistry, ModelTier
import asyncio

class Executor:
    def __init__(self):
        self.gemini = GeminiClient()
        self.anthropic = AnthropicClient()
        self.registry = ModelRegistry()

    async def execute(
        self,
        model_tier: ModelTier,
        prompt: str,
        max_tokens: int = 4096,
        retry_count: int = 2
    ) -> ExecutionResult:
        """Execute task on specified model with retry logic."""

        # Select client based on tier
        if model_tier in [ModelTier.FLASH, ModelTier.PRO]:
            client = self.gemini
            model = "gemini-flash" if model_tier == ModelTier.FLASH else "gemini-pro"
        else:
            client = self.anthropic
            model = {
                ModelTier.HAIKU: "haiku",
                ModelTier.SONNET: "sonnet",
                ModelTier.OPUS: "opus"
            }[model_tier]

        # Retry loop
        for attempt in range(retry_count + 1):
            result = await client.execute(prompt, max_tokens)

            if result.success:
                return result

            # If not last attempt, wait and retry
            if attempt < retry_count:
                await asyncio.sleep(2 ** attempt)  # exponential backoff

        # All retries failed - fallback to next tier up
        if model_tier == ModelTier.FLASH:
            return await self.execute(ModelTier.HAIKU, prompt, max_tokens, retry_count=0)

        # Return final failure
        return result

    async def validate_output(self, task: str, output: str) -> dict:
        """Always use Sonnet for validation (input-heavy mode)."""
        client = AnthropicClient(model="sonnet")
        return await client.validate(task, output)
```

Use AuZoom for reference: Check auzoom/src/auzoom/core/caching/ (via auzoom_read) for retry pattern examples.

Keep ≤250 lines. Test retry logic with mock clients that fail then succeed.
  </action>
  <verify>pytest tests/test_executor.py passes, retry logic works (fails twice then succeeds), fallback from Flash→Haiku tested, validation returns proper structure</verify>
  <done>Unified executor with retry/fallback complete, all model tiers supported, validation method working, comprehensive test coverage</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] All clients implement ModelClient interface correctly
- [ ] Gemini CLI wrapper handles missing CLI gracefully (doesn't crash tests)
- [ ] Anthropic client uses proper model IDs from registry
- [ ] Retry logic works with exponential backoff
- [ ] Fallback logic prevents infinite loops
- [ ] All tests pass without requiring actual API keys/CLI
- [ ] Code follows AuZoom standards (≤250 lines/module)
</verification>

<success_criteria>
- All tasks completed
- All tests pass (10+ client tests, 5+ executor tests)
- Both Gemini and Anthropic clients functional
- Unified interface works seamlessly
- Retry and fallback logic tested
- Ready for MCP server integration (Plan 02-03)
</success_criteria>

<output>
After completion, create `.planning/phases/02-orchestrator-implementation/02-02-SUMMARY.md`:

# Phase 2 Plan 02: Model Dispatch Layer Summary

**[One-liner describing what shipped]**

## Accomplishments

- Gemini CLI wrapper with subprocess execution
- Anthropic API client with all 3 models (Haiku/Sonnet/Opus)
- Unified ModelClient interface and ExecutionResult
- Retry logic with exponential backoff
- Fallback routing (Flash→Haiku on failure)
- Validation method for Sonnet-based checkpoints

## Files Created/Modified

- `orchestrator/src/orchestrator/clients/base.py` - Shared interfaces
- `orchestrator/src/orchestrator/clients/gemini.py` - Gemini CLI client
- `orchestrator/src/orchestrator/clients/anthropic.py` - Anthropic API client
- `orchestrator/src/orchestrator/executor.py` - Unified executor with retry/fallback
- `orchestrator/tests/test_*.py` - Test coverage
- `orchestrator/pyproject.toml` - Updated dependencies

## Commits

- [List commit SHAs and messages]

## Decisions Made

[Key decisions and rationale]

## Issues Encountered

[Problems and resolutions, or "None"]

## Next Step

Ready for 02-03-PLAN.md (MCP Server & Integration)
</output>
