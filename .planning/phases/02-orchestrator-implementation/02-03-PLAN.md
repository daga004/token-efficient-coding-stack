---
phase: 02-orchestrator-implementation
plan: 03
type: execute
---

<objective>
Create MCP server exposing orchestrator tools (route, execute, validate) for Claude Code integration.

Purpose: Enable Claude Code to leverage the orchestrator through three MCP tools: orchestrator_route (get routing recommendation), orchestrator_execute (run task on specified model), orchestrator_validate (Sonnet-based validation checkpoint).

Output: Working MCP server with 3 tools, configuration file for model endpoints, end-to-end integration tests, ready for GSD workflow integration in Phase 3.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-phase.md
@~/.claude/get-shit-done/templates/summary.md
@~/.claude/get-shit-done/references/checkpoints.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-orchestrator-implementation/02-01-SUMMARY.md
@.planning/phases/02-orchestrator-implementation/02-02-SUMMARY.md

**Existing code from Plans 02-01 and 02-02:**
- Use `auzoom_read(path="orchestrator/src/orchestrator/", level="skeleton")` to see full module structure
- Use `auzoom_read(path="orchestrator/src/orchestrator/scoring.py", level="summary")` for complexity scorer
- Use `auzoom_read(path="orchestrator/src/orchestrator/executor.py", level="summary")` for execution interface

**Reference AuZoom MCP implementation:**
- Use `auzoom_read(path="auzoom/src/auzoom/mcp/server.py", level="summary")` to understand MCP server patterns
- Use `auzoom_read(path="auzoom/src/auzoom/mcp/tools_schema.py", level="summary")` to understand tool schema structure
- Use `auzoom_read(path="auzoom/src/auzoom/mcp/jsonrpc_handler.py", level="summary")` for JSON-RPC patterns

**MCP Integration Strategy (from research):**
- **orchestrator_route**: Returns {model, reason, confidence} - recommendation only, doesn't execute
- **orchestrator_execute**: Takes {model, prompt, max_tokens} - executes on specified model
- **orchestrator_validate**: Takes {task, output} - Sonnet validation, returns {pass, issues, confidence, escalate}

**Constraining decisions:**
- Follow AuZoom's MCP server architecture (proven working)
- Configuration file for API keys (orchestrator/config.json)
- Async execution for all tools
- Follow AuZoom standards (≤250 lines/module)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create MCP server with orchestrator_route tool</name>
  <files>orchestrator/src/orchestrator/mcp/server.py, orchestrator/src/orchestrator/mcp/tools_schema.py, orchestrator/src/orchestrator/mcp/__init__.py</files>
  <action>
Create MCP package structure following AuZoom patterns:
```
orchestrator/src/orchestrator/mcp/
├── __init__.py
├── server.py         # Main MCP server
├── tools_schema.py   # Tool definitions
└── jsonrpc_handler.py  # JSON-RPC protocol
```

Use AuZoom's MCP server as template: Read auzoom/src/auzoom/mcp/server.py (via auzoom_read at summary level) and adapt patterns.

In tools_schema.py, define:
```python
def get_tools_manifest():
    return {
        "tools": [
            {
                "name": "orchestrator_route",
                "description": "Get routing recommendation for a task based on complexity scoring",
                "inputSchema": {
                    "type": "object",
                    "properties": {
                        "task": {"type": "string", "description": "Task description"},
                        "context": {
                            "type": "object",
                            "properties": {
                                "files_count": {"type": "integer"},
                                "requires_tests": {"type": "boolean"},
                                "subsystems": {"type": "array", "items": {"type": "string"}}
                            }
                        }
                    },
                    "required": ["task"]
                }
            }
            # More tools in next tasks
        ]
    }
```

In server.py, implement OrchestratorMCPServer:
```python
from ..scoring import score_complexity
from ..registry import ModelRegistry
from ..executor import Executor

class OrchestratorMCPServer:
    def __init__(self):
        self.registry = ModelRegistry()
        self.executor = Executor()

    async def handle_tool_call(self, tool_name: str, arguments: dict) -> dict:
        if tool_name == "orchestrator_route":
            return await self._route(arguments)
        # More tools...

    async def _route(self, args: dict) -> dict:
        task = args.get("task")
        context = args.get("context", {})

        complexity = score_complexity(task, context)
        model = self.registry.get_model_for_score(complexity.score)
        profile = self.registry.models[model]

        return {
            "model": model.value,
            "complexity_score": complexity.score,
            "complexity_factors": complexity.factors,
            "reason": f"Task scored {complexity.score}/10. Recommended: {profile.name}",
            "confidence": complexity.confidence,
            "estimated_cost": {
                "model": profile.name,
                "cost_per_1m_input": profile.cost_per_1m_input,
                "cost_per_1m_output": profile.cost_per_1m_output
            }
        }
```

Use AuZoom's jsonrpc_handler.py as reference for protocol handling (via auzoom_read).

Keep modules ≤250 lines each. Test with mock tool calls.
  </action>
  <verify>pytest tests/test_mcp_server.py passes, orchestrator_route returns proper structure with model recommendation, complexity breakdown, cost estimate</verify>
  <done>MCP server scaffold created following AuZoom patterns, orchestrator_route tool working, proper JSON-RPC protocol handling, tests passing</done>
</task>

<task type="auto">
  <name>Task 2: Implement orchestrator_execute and orchestrator_validate tools</name>
  <files>orchestrator/src/orchestrator/mcp/server.py, orchestrator/src/orchestrator/mcp/tools_schema.py</files>
  <action>
Add to tools_schema.py:
```python
{
    "name": "orchestrator_execute",
    "description": "Execute task on specified model (Gemini Flash/Pro, Haiku, Sonnet, Opus)",
    "inputSchema": {
        "type": "object",
        "properties": {
            "model": {
                "type": "string",
                "enum": ["gemini-flash", "gemini-pro", "haiku", "sonnet", "opus"],
                "description": "Model to use for execution"
            },
            "prompt": {"type": "string", "description": "Task prompt"},
            "max_tokens": {"type": "integer", "default": 4096}
        },
        "required": ["model", "prompt"]
    }
},
{
    "name": "orchestrator_validate",
    "description": "Validate output using Sonnet (input-heavy validation checkpoint)",
    "inputSchema": {
        "type": "object",
        "properties": {
            "task": {"type": "string", "description": "Original task description"},
            "output": {"type": "string", "description": "Output to validate"}
        },
        "required": ["task", "output"]
    }
}
```

In server.py, add methods:
```python
async def _execute(self, args: dict) -> dict:
    model_name = args.get("model")
    prompt = args.get("prompt")
    max_tokens = args.get("max_tokens", 4096)

    # Map model name to tier
    model_map = {
        "gemini-flash": ModelTier.FLASH,
        "gemini-pro": ModelTier.PRO,
        "haiku": ModelTier.HAIKU,
        "sonnet": ModelTier.SONNET,
        "opus": ModelTier.OPUS
    }

    model_tier = model_map.get(model_name)
    if not model_tier:
        return {"error": f"Unknown model: {model_name}"}

    result = await self.executor.execute(model_tier, prompt, max_tokens)

    return {
        "success": result.success,
        "response": result.response,
        "tokens": {
            "input": result.tokens_input,
            "output": result.tokens_output
        },
        "latency_ms": result.latency_ms,
        "error": result.error
    }

async def _validate(self, args: dict) -> dict:
    task = args.get("task")
    output = args.get("output")

    validation = await self.executor.validate_output(task, output)

    return {
        "pass": validation.get("pass", False),
        "issues": validation.get("issues", []),
        "confidence": validation.get("confidence", 0),
        "escalate": validation.get("escalate", False)
    }
```

Update handle_tool_call to route to new methods.

Keep server.py ≤250 lines. Test all 3 tools with integration tests.
  </action>
  <verify>pytest tests/test_mcp_integration.py passes, all 3 tools (route/execute/validate) work end-to-end, execute returns proper ExecutionResult, validate returns proper validation structure</verify>
  <done>All 3 MCP tools implemented and tested, execute handles all 5 models, validate uses Sonnet correctly, proper error handling throughout</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>MCP server configuration and end-to-end integration test</what-built>
  <how-to-verify>
I'll create configuration file and test the MCP server:

1. Create `orchestrator/config.json`:
```json
{
  "anthropic_api_key": "${ANTHROPIC_API_KEY}",
  "gemini_cli_path": "gemini",
  "default_max_tokens": 4096,
  "retry_attempts": 2,
  "timeout_seconds": 30
}
```

2. Create `.claude/settings.json` entry for orchestrator MCP server:
```json
{
  "mcp": {
    "servers": {
      "orchestrator": {
        "command": "python3",
        "args": ["-m", "orchestrator.mcp.server"],
        "cwd": "/Users/dhirajd/Documents/claude/orchestrator",
        "env": {
          "PYTHONPATH": "/Users/dhirajd/Documents/claude/orchestrator/src",
          "ANTHROPIC_API_KEY": "${ANTHROPIC_API_KEY}"
        }
      }
    }
  }
}
```

3. Run integration test:
```bash
cd orchestrator
pytest tests/test_mcp_integration.py -v
```

4. Test MCP server manually (if you want to verify):
```bash
# Start server
python3 -m orchestrator.mcp.server

# In another terminal, test with MCP client
# (Or wait for Phase 3 GSD integration)
```

Verify:
- [ ] All integration tests pass
- [ ] Configuration file created
- [ ] MCP server starts without errors
- [ ] All 3 tools callable
- [ ] No import errors

Type **"approved"** if tests pass and server starts correctly, or describe any issues to fix.
  </how-to-verify>
  <resume-signal>Type "approved" to continue, or describe issues to fix</resume-signal>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] All 3 MCP tools implemented and tested
- [ ] Server follows AuZoom MCP patterns (JSON-RPC, tool schema, error handling)
- [ ] Configuration file supports environment variables
- [ ] Integration tests pass without requiring actual API keys
- [ ] Server can start and respond to tool calls
- [ ] Code follows AuZoom standards (≤250 lines/module)
- [ ] Documentation includes MCP server setup instructions
</verification>

<success_criteria>
- All tasks completed
- All tests pass (10+ unit tests, 5+ integration tests)
- MCP server implements all 3 tools correctly
- Configuration system working
- Server ready for Claude Code integration
- Phase 2 complete - ready for Phase 3 (GSD Integration)
</success_criteria>

<output>
After completion, create `.planning/phases/02-orchestrator-implementation/02-03-SUMMARY.md`:

# Phase 2 Plan 03: MCP Server & Integration Summary

**[One-liner describing what shipped]**

## Accomplishments

- MCP server with 3 tools (route, execute, validate)
- Tool schema definitions following MCP protocol
- JSON-RPC protocol handling
- Configuration file system with environment variable support
- End-to-end integration tests
- MCP server configuration for Claude Code

## Files Created/Modified

- `orchestrator/src/orchestrator/mcp/server.py` - Main MCP server
- `orchestrator/src/orchestrator/mcp/tools_schema.py` - Tool definitions
- `orchestrator/src/orchestrator/mcp/jsonrpc_handler.py` - Protocol handling
- `orchestrator/config.json` - Configuration template
- `orchestrator/tests/test_mcp_*.py` - Integration tests
- Documentation for MCP setup

## Commits

- [List commit SHAs and messages]

## Decisions Made

[Key decisions and rationale]

## Issues Encountered

[Problems and resolutions, or "None"]

## Next Phase

Phase 2 complete! Ready for Phase 3 (Integration & Validation) - wire orchestrator and AuZoom into GSD execution flow.
</output>
