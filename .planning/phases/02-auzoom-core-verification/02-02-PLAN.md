---
phase: 02-auzoom-core-verification
plan: 02
type: execute
domain: auzoom-structured-code
---

<objective>
Verify dependency tracking correctness - test that auzoom_get_dependencies accurately identifies function-level dependencies in real Python code.

Purpose: Validate Assumption 1 dependency component - accurate dependency graphs are essential for targeted context loading.
Output: Test results showing dependency tracking precision and recall with evidence of correctness.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Phase 2 prior work
@.planning/phases/02-auzoom-core-verification/02-01-SUMMARY.md

# Audit infrastructure
@audit/audit_harness.py
@audit/templates/test_template.py

# AuZoom implementation
@auzoom/src/auzoom/graph.py
@auzoom/src/auzoom/tools.py

**Established patterns from Phase 1:**
- Evidence-based audit with file:line citations
- AuditTest base class for test structure
- JSON Lines evidence collection

**Key decisions:**
- Dependency tracking is core to Assumption 1 (targeted context loading)
- Must verify both direct and transitive dependencies
- Must test on real codebase (auzoom/ and orchestrator/)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create dependency tracking accuracy test</name>
  <files>audit/tests/test_dependency_tracking.py</files>
  <action>Create test using AuditTest base class to verify auzoom_get_dependencies correctness. Test approach: select 5-8 functions from auzoom/ and orchestrator/ with known dependencies (imports, function calls). For each function, manually identify expected dependencies (direct dependencies only - functions called within the function body). Use auzoom_get_dependencies with depth=1 to get actual dependencies. Compare actual vs expected. Calculate precision (% of returned deps that are correct) and recall (% of expected deps that were found). Use tree-sitter AST to parse functions and identify call sites for ground truth. Record evidence: node_id, expected deps, actual deps, precision, recall, false positives, false negatives. Do NOT use mocks - test real dependency graph. Avoid assuming 100% accuracy - calculate real metrics.</action>
  <verify>pytest audit/tests/test_dependency_tracking.py -v passes, evidence JSON Lines written with precision/recall metrics</verify>
  <done>Test executable, measures accuracy for 5+ functions, precision and recall calculated, evidence collected with specific dependency lists</done>
</task>

<task type="auto">
  <name>Task 2: Execute dependency tracking test and analyze accuracy</name>
  <files>audit/evidence/dependency_tracking_*.jsonl, audit/reports/02-02-dependency-tracking.md</files>
  <action>Run test with pytest -v. Capture evidence to JSON Lines file. Analyze results: calculate average precision and recall across all tested functions. Identify categories of errors: false positives (reported but not actual dependencies), false negatives (missed dependencies), transitive dependency handling. Create markdown report with: test methodology, functions tested (file:line references), precision and recall by function, average metrics, error analysis with examples, assessment of whether accuracy is sufficient for "targeted context loading" claim. If accuracy <90%, document impact on core assumption (does inaccurate dependency tracking undermine token reduction benefits?).</action>
  <verify>Report exists at audit/reports/02-02-dependency-tracking.md, evidence file has precision/recall for all test cases, report includes assessment of accuracy sufficiency</verify>
  <done>Evidence collected, report written with quantitative metrics (avg precision, avg recall), error categories documented, impact on Assumption 1 assessed</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] Test runs successfully with pytest
- [ ] Dependency accuracy measured for 5+ functions
- [ ] Precision and recall metrics calculated
- [ ] Evidence JSON Lines file contains actual vs expected dependencies
- [ ] Report documents accuracy findings with file:line citations
- [ ] Assessment of whether accuracy supports "targeted context loading" claim
</verification>

<success_criteria>

- Test implemented using AuditTest base class
- Dependency tracking accuracy measured on real functions
- Evidence collected with precision/recall metrics for 5+ test cases
- Report documents whether dependency tracking is accurate enough for core assumption
- No errors or warnings introduced
  </success_criteria>

<output>
After completion, create `.planning/phases/02-auzoom-core-verification/02-02-SUMMARY.md`:

# Phase 2 Plan 02: Dependency Tracking Accuracy Summary

**[Substantive one-liner - precision/recall findings]**

## Accomplishments

- Created dependency tracking accuracy test with ground truth comparison
- Tested X functions across auzoom/ and orchestrator/ codebases
- Calculated precision and recall metrics (average: X% / Y%)
- Assessed impact on "targeted context loading" assumption

## Files Created/Modified

- `audit/tests/test_dependency_tracking.py` - Accuracy test
- `audit/evidence/dependency_tracking_*.jsonl` - Test evidence
- `audit/reports/02-02-dependency-tracking.md` - Accuracy findings

## Decisions Made

[Assessment of whether dependency tracking accuracy is sufficient, identification of error patterns]

## Issues Encountered

[Problems and resolutions, or "None"]

## Next Step

Ready for 02-03-PLAN.md (bypass behavior testing)
</output>
