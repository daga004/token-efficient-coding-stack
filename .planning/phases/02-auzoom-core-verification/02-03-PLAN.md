---
phase: 02-auzoom-core-verification
plan: 03
type: execute
domain: auzoom-structured-code
---

<objective>
Test for bypass behavior - verify that full reads are not used when skeleton/summary levels should suffice.

Purpose: Ensure progressive disclosure is actually being used, not bypassed due to implementation issues or cache misses.
Output: Evidence showing when each disclosure level is used and whether full reads are inappropriately triggered.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Phase 2 prior work
@.planning/phases/02-auzoom-core-verification/02-01-SUMMARY.md
@.planning/phases/02-auzoom-core-verification/02-02-SUMMARY.md

# Audit infrastructure
@audit/audit_harness.py
@audit/templates/test_template.py

# AuZoom implementation
@auzoom/src/auzoom/tools.py
@auzoom/src/auzoom/graph.py
@auzoom/src/auzoom/cache.py

**Established patterns from Phase 1:**
- Evidence-based audit with file:line citations
- AuditTest base class for test structure
- JSON Lines evidence collection

**Key decisions:**
- Progressive disclosure only provides value if actually used (not bypassed)
- Must verify cache behavior (are cached files re-read at full level?)
- Must test that appropriate level is used for different query types
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create bypass behavior detection test</name>
  <files>audit/tests/test_bypass_behavior.py</files>
  <action>Create test using AuditTest base class to detect inappropriate full reads. Test scenarios: (1) Call auzoom_find to locate a function, verify it returns skeleton-level data not full file content. (2) Call auzoom_get_dependencies, verify it uses cached skeleton/summary not fresh full read. (3) Request same file at skeleton then summary then full - verify cache is used (not re-parsed each time). (4) Simulate common usage: find function → get dependencies → read summary - verify no full reads triggered. Instrument by: checking return data size (skeleton < summary < full), examining cache stats via auzoom_stats before/after operations (cache hits vs misses), checking if returned content matches expected level (skeleton has signatures only, summary adds metadata, full has implementation). Record evidence: operation sequence, level requested, level actually returned, cache hit/miss, data size, inappropriateness indicator (yes/no). Do NOT use mocks - test real MCP tool behavior.</action>
  <verify>pytest audit/tests/test_bypass_behavior.py -v passes, evidence JSON Lines written with bypass detection results</verify>
  <done>Test executable, covers 4+ usage scenarios, detects whether full reads inappropriately triggered, cache behavior verified, evidence collected</done>
</task>

<task type="auto">
  <name>Task 2: Execute bypass test and analyze cache/disclosure behavior</name>
  <files>audit/evidence/bypass_behavior_*.jsonl, audit/reports/02-03-bypass-behavior.md</files>
  <action>Run test with pytest -v. Capture evidence to JSON Lines file. Analyze results: identify any scenarios where full reads occurred when skeleton/summary should have sufficed. Check cache hit rates (should be high for repeated access). Verify level escalation only happens when explicitly requested (not automatic fallback to full). Create markdown report with: test methodology, scenarios tested (find/dependencies/repeated access/common workflows), bypass incidents table (scenario, expected level, actual level, bypass: yes/no), cache statistics (hit rate, miss reasons), assessment of whether progressive disclosure is consistently applied. If bypasses detected, document root cause (implementation bug, cache invalidation issue, level selection logic error) and impact on token reduction claims.</action>
  <verify>Report exists at audit/reports/02-03-bypass-behavior.md, evidence file has bypass detection for all scenarios, report includes cache hit rate and bypass incident analysis</verify>
  <done>Evidence collected, report written documenting bypass incidents (if any), cache behavior analyzed, impact on progressive disclosure assumption assessed</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] Test runs successfully with pytest
- [ ] Bypass behavior tested across 4+ scenarios
- [ ] Cache statistics captured (hit/miss rates)
- [ ] Evidence JSON Lines file contains level usage data
- [ ] Report documents whether progressive disclosure is consistently applied
- [ ] Root cause analysis if bypasses detected
</verification>

<success_criteria>

- Test implemented using AuditTest base class
- Bypass detection tested on real MCP tool operations
- Evidence collected for multiple usage scenarios
- Report documents whether full reads are inappropriately used
- Cache behavior verified and documented
- No errors or warnings introduced
  </success_criteria>

<output>
After completion, create `.planning/phases/02-auzoom-core-verification/02-03-SUMMARY.md`:

# Phase 2 Plan 03: Bypass Behavior Detection Summary

**[Substantive one-liner - bypass incidents and cache hit rate findings]**

## Accomplishments

- Created bypass detection test covering common usage scenarios
- Tested X scenarios for inappropriate full reads
- Measured cache hit rate (X%)
- Identified bypass incidents (if any) with root cause

## Files Created/Modified

- `audit/tests/test_bypass_behavior.py` - Bypass detection test
- `audit/evidence/bypass_behavior_*.jsonl` - Test evidence
- `audit/reports/02-03-bypass-behavior.md` - Bypass analysis

## Decisions Made

[Assessment of whether progressive disclosure is consistently applied, cache effectiveness]

## Issues Encountered

[Problems and resolutions, or "None"]

## Next Step

Ready for 02-04-PLAN.md (actual token savings measurement on real codebases)
</output>
